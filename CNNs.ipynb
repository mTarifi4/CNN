{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d58a9204-38a1-4066-bff8-80744ddf714f",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "- **Part I:  Implmentation of a feed-forward neural network**\n",
    "    - Build your feed-forward network with different layers and activation functions\n",
    "    - Define the gradient descent function to update the parameters\n",
    "    - Adjust the learning rate to achieve better performance \n",
    "    - Run the evaluation function\n",
    "\n",
    "\n",
    "- **Part II: implement your a Convolutional Neural Network**\n",
    "    - Train the CNN and compare it with the feed-forward neural network\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c51b6-d1cc-486e-bae9-1e159c2c959a",
   "metadata": {},
   "source": [
    "\n",
    "Let's first import all the packages that you will need.\n",
    "\n",
    "- **torch, torch.nn, torch.nn.functional** are the fundamental modules in pytorch library, supporting Python programs that facilitates building deep learning projects.\n",
    "- **torchvision** is a library for Computer Vision that goes hand in hand with PyTorch\n",
    "- **numpy** is the fundamental package for scientific computing with Python programs.\n",
    "- **matplotlib** is a library to plot graphs and images in Python.\n",
    "- **math, random** are the standard modules in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9777d3e4-091f-4e4b-9911-9fd29404cb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import packages successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from project1_utils import *\n",
    "\n",
    "print(\"Import packages successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7abe3da-43e3-4143-b7d0-160259bf6980",
   "metadata": {},
   "source": [
    "A helper function is provided:\n",
    "\n",
    "```python\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Use random seed to ensure that results are reproducible.\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10feaf7a-2161-4a82-9345-f09201c7813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 265\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a8fcb-5b9f-45a1-a85a-9e82751e3319",
   "metadata": {},
   "source": [
    "##  Dataset\n",
    "\n",
    "Let's load the dataset first using pytorch dataset and loader modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "579e0a5e-471b-4c6e-a5e3-321c1f5d9c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 1034\n",
      "Number of testing examples: 126\n"
     ]
    }
   ],
   "source": [
    "# the number of images in a batch\n",
    "batch_size = 8\n",
    "\n",
    "# load dataset\n",
    "trainset = dataset(path='use the path of your training set')\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testset = dataset(path='use the path of your testing set')\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "# name of classes\n",
    "classes = (\"define your classes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a55eb-bab5-40b2-a5eb-f6bd5593f080",
   "metadata": {},
   "source": [
    "Let's visualize some examples in the dataset, the tool to show images is provided as below:\n",
    "\n",
    "```python\n",
    "def imshow(images):\n",
    "    \"\"\"\n",
    "    Display the input images in a plot\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c64438-80ae-4f0f-a6dc-772060b6cb1a",
   "metadata": {},
   "source": [
    "# Part I\n",
    "\n",
    "---\n",
    "\n",
    "## Feed-forward neural network.\n",
    "\n",
    "In this cell, we will build a **four-layer multilayer perceptron (MLP)** to classify images into different categories. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1749d127-6788-4b2b-b196-6b0199b81fad",
   "metadata": {},
   "source": [
    "The size of input images is a batch-like tensor $ X \\in \\mathbb{R}^{B \\times C \\times H \\times W}$, where $B$ denotes the batch size. Vectorize the image pixels equals to transforming into a vector $X_{vector} \\in \\mathbb{R}^{B \\times CHW}$.\n",
    "\n",
    "To process image data with feed-forward neural network, the image pixels should be vectorized. So first, we implement a vectorized function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab75f972-dbd8-4347-9533-65613eca7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_vectorization(image_batch):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        image_batch: a batch of images with shape [b, c, h, w]\n",
    "    Output: \n",
    "        vectorized_image_batch: a batch of neurons\n",
    "    \"\"\"\n",
    "    vectorized_image_batch = image_batch.view(image_batch.size(0), -1) # change shape to R^(B, CHW) \n",
    "    return vectorized_image_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dda7400-a302-4267-b5c5-e1c531b47e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IGNORE: testing \n",
    "# def test_vect(image_batch):\n",
    "#     print(f\"this is my size: {image_batch.size()}\") \n",
    "\n",
    "# B1 = 10  # Batch size of 10 images\n",
    "# X_batch1 = torch.randn(B1, 1, 64, 64)\n",
    "# test_vect(X_batch1)\n",
    "\n",
    "# vectorized = image_vectorization(X_batch1)\n",
    "# print(f\"this is my size after vect: {vectorized.size()}\") \n",
    "\n",
    "# # this is my size: torch.Size([10, 1, 64, 64])\n",
    "# # this is my size after vect: torch.Size([10, 4096])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded2308-5a78-4887-8632-1a2e33b0c2df",
   "metadata": {},
   "source": [
    " **each layer** of a MLP can be denoted as the following mathematical operation:\n",
    "\n",
    "$$z = W^T x + b$$ \n",
    "\n",
    "Here, $W, b$ denote the weights and biases. The function is **parameterized by $W, b$**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "291908ba-1506-4692-bb92-4b4c70cafd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "torch.randn(input_dim, output_dim): This creates a tensor of the specified shape (input_dim x output_dim) \n",
    "filled with random numbers drawn from a standard normal distribution (mean 0 and variance 1).\n",
    "\n",
    "torch.sqrt(torch.tensor(2.0) / (input_dim + output_dim)): This calculates the scaling factor. \n",
    "For the Xavier initialization with a sigmoid or tanh activation, the weights are scaled by the square root of \n",
    "2/ (number of input units + number of output units)\n",
    "\n",
    "Multiplying the random tensor by the scaling factor ensures that the weights are initialized \n",
    "in a manner that helps combat the vanishing/exploding gradient problem in deep networks.\n",
    "\"\"\"\n",
    "def get_layer_params(input_dim: int, output_dim: int):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        input_dim: number of neurons in the input\n",
    "        output_dim: number of neurons produced by the layer\n",
    "    Output: \n",
    "        a dictionary of generated parameters\n",
    "            - w: weights\n",
    "            - b: biases\n",
    "    \"\"\"\n",
    "    \n",
    "    # Xavier/Glorot initialization for weights\n",
    "    w = torch.randn(input_dim, output_dim, requires_grad=True) #* torch.sqrt(torch.tensor(2.0) / (input_dim + output_dim))\n",
    "    \n",
    "    # Biases initialization (zeros)\n",
    "    b = torch.zeros(output_dim,requires_grad=True)\n",
    "    \n",
    "    return {'w': w,\n",
    "            'b': b}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d46f2-9c7c-484d-9c8f-574054877635",
   "metadata": {},
   "source": [
    "Following with the previous linear layer, an activation layer is required to add non-linearity to the network:\n",
    "\n",
    " $$a = \\sigma(z)$$\n",
    "\n",
    " $a, \\sigma$ denote activation output and activation function, respectively.\n",
    "The entire layer function is also **parameterized by choice of $\\sigma(\\cdot)$**.\n",
    "\n",
    "**Question 3 (4-3 points):** We need an activation wrapper function to support the following three activation functions.\n",
    "- Sigmoid\n",
    "- tanh\n",
    "- ReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5338b533-87dc-4953-ba12-e73c4389ad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_wrapper(x, activation='relu'):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        x: the input neuron values\n",
    "        activation: name of activation, could be one in ['relu', 'sigmoid', 'tanh']\n",
    "    Output: \n",
    "        a: the corresponding activated output\n",
    "    \"\"\"\n",
    "\n",
    "    if activation == 'relu':\n",
    "        # use ReLU(x) = max(0, x) this zeros out all negative values by doing a comparison etween elemnts in x and \n",
    "        # an all-zeros tensor of the same size as x  \n",
    "        a = torch.max(torch.tensor(0.0),x)\n",
    "\n",
    "        \"\"\"\n",
    "        It is computationally expensive, causes vanishing gradient problem and not zero-centred. \n",
    "        This method is generally used for binary classification problems.\n",
    "        \"\"\"\n",
    "    elif activation == 'sigmoid':\n",
    "        # formula: sigmoid(x) = 1/(1 + exp(-x))\n",
    "        a = 1 / (1 + torch.exp(-x))\n",
    "    \n",
    "        \"\"\"\n",
    "        If you compare it to sigmoid, it solves just one problem of being zero-centred.\n",
    "        \"\"\"\n",
    "    elif activation == 'tanh':\n",
    "        \n",
    "        # Tanh(x) = (exp(x) - exp(-x))/(exp(x) + exp(-x))\n",
    "        a = (torch.exp(x) - torch.exp(-x)) / (torch.exp(x) + torch.exp(-x))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    return a "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb4179d-2d0f-4ea7-92a2-22f9a1e2cca2",
   "metadata": {},
   "source": [
    "Given the layer parameters $W, b$ and the choice of $\\sigma(\\cdot)$, compute the output for an MLP layer with input $x$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06d87761-5da8-4205-8d20-731252c4bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_forward_computation(x, params, activation):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        x: the input to the layer\n",
    "        params: parameters of each layer\n",
    "        activation: activation type\n",
    "    Output: \n",
    "        a: the output after the activation\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # compute the output for layer\n",
    "    Z = x.mm(params[\"w\"] ) + params[\"b\"]\n",
    "\n",
    "    # apply activation\n",
    "    a = activation_wrapper(Z, activation)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12de9e3-ace3-44e4-ac8a-032cee88a456",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "**Architecture **:\n",
    "\n",
    "We now describe in details how our four-layer MLP should be built in PyTorch.\n",
    "\n",
    "1. In the dataset, the size of input image is a tensor $ X \\in \\mathbb{R}^{B \\times 1 \\times 64 \\times 64}$, where $B$ denotes the batch size.\n",
    "2. Vectorize the image pixels to a vector $X_{vector} \\in \\mathbb{R}^{B \\times 4096}$.\n",
    "3. We now begin describing the specific architecture of the model, although this is not the only design choice, and feel free to change the hidden dimensions of the parameters\n",
    "4. Layer1: set your parameters so the input is projected from $\\mathbb{R}^{B \\times 4096}$ to $\\mathbb{R}^{B \\times 2048}$, use ReLU as your activation function\n",
    "5. Layer2: set your parameters so the input is projected from $\\mathbb{R}^{B \\times 2048}$ to $\\mathbb{R}^{B \\times 1024}$, use ReLU as your activation function\n",
    "6. Layer3: set your parameters so the input is projected from $\\mathbb{R}^{B \\times 1024}$ to $\\mathbb{R}^{B \\times 256}$, use ReLU as your activation function\n",
    "7. Layer4: set your parameters so the input is projected from $\\mathbb{R}^{B \\times 256}$ to $\\mathbb{R}^{B \\times 2}$, use sigmoid function as your activation function\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93925494-0ae8-4668-82a1-d13b0b1da3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_params: dict = dict()\n",
    "layer2_params: dict = dict()\n",
    "layer3_params: dict = dict()\n",
    "layer4_params: dict = dict()\n",
    "\n",
    "def net(X, params, activations):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        X: the input images to the network\n",
    "        params: a dictionary of parameters(W and b) for the four different layers\n",
    "        activations: a dictionary of activation function names for the four different layers\n",
    "    Output: \n",
    "        output: the final output from the four layer\n",
    "    \"\"\"\n",
    "    \n",
    "    # build your network forward\n",
    "    # 1- vectorize image\n",
    "    X_vectorized = image_vectorization(X)\n",
    "\n",
    "    # 2- Forward Pass Layer 1 \n",
    "    A1 = layer_forward_computation(X_vectorized, params[\"layer1\"], activations[\"layer1\"])\n",
    "    \n",
    "    # 3- Forward pass through Layer 2\n",
    "    A2 = layer_forward_computation(A1, params['layer2'], activations['layer2'])\n",
    "    \n",
    "    # 4- Forward pass through Layer 3\n",
    "    A3 = layer_forward_computation(A2, params['layer3'], activations['layer3'])\n",
    "    \n",
    "    # 5- Forward pass through Layer 4\n",
    "    output = layer_forward_computation(A3, params['layer4'], activations['layer4'])\n",
    "    \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aac2bdc8-3d8c-40c3-83c4-f338bcd50552",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We prepare serval dictories to store the parameters and activations for different   \"\"\"\n",
    "layer1_params: dict = dict()\n",
    "layer2_params: dict = dict()\n",
    "layer3_params: dict = dict()\n",
    "layer4_params: dict = dict()\n",
    "params: dict = dict()\n",
    "activations: dict = dict()\n",
    "\n",
    "\n",
    "# Define layer parameters using the get_layer_params function\n",
    "layer1_params = get_layer_params(4096, 2048) \n",
    "layer2_params = get_layer_params(2048, 1024)\n",
    "layer3_params = get_layer_params(1024, 256)\n",
    "layer4_params = get_layer_params(256, 2)\n",
    "\n",
    "# Pack layer parameters into the params dictionary\n",
    "params['layer1'] = layer1_params\n",
    "params['layer2'] = layer2_params\n",
    "params['layer3'] = layer3_params\n",
    "params['layer4'] = layer4_params\n",
    "\n",
    "# Define activations for each layer\n",
    "activations['layer1'] = 'relu'\n",
    "activations['layer2'] = 'relu'\n",
    "activations['layer3'] = 'relu'\n",
    "activations['layer4'] = 'sigmoid'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4653f366-9db8-48df-8c1b-3a31a8bf823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #IGNORE: testing\n",
    "\n",
    "# # Test case 1\n",
    "# B1 = 10  # Batch size of 10 images\n",
    "# X_batch1 = torch.randn(B1, 1, 64, 64)\n",
    "# output1 = net(X_batch1, params, activations)\n",
    "# print(f\"Test case 1 - Output shape: {output1.shape}\")  # Expected shape: [10, 2]\n",
    "\n",
    "# # Test case 2\n",
    "# B2 = 5  # Batch size of 5 images\n",
    "# X_batch2 = torch.randn(B2, 1, 64, 64)\n",
    "# output2 = net(X_batch2, params, activations)\n",
    "# print(f\"Test case 2 - Output shape: {output2.shape}\")  # Expected shape: [5, 2]\n",
    "\n",
    "# # Test case 1 - Output shape: torch.Size([10, 2])\n",
    "# # Test case 2 - Output shape: torch.Size([5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2af561-02ac-400b-8c7b-b623b4774f87",
   "metadata": {},
   "source": [
    "##  Backpropagation and optimization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7b9551-a406-4fb0-94bb-7e869746ba14",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Gradient descent is a way to minimize the final objective function (loss) parameterized by a model's parameter $\\theta$ by updating the parameters in the opposite direction of the gradient $\\nabla_\\theta J(\\theta)$ w.r.t to the parameters. The learning rate $\\lambda$ determines the size of the steps you take to reach a (local) minimum.\n",
    "\n",
    "However, for the vanilla gradient descent, you need to run through all the samples in your training set and update once. This will be time-consuming with large-scale datasets. We choose Stochastic Gradient Descent, which only requires a subset of training samples to update the parameters. With the popular deep learning framework, the subset usually equals to the minibatch selected during training.\n",
    "\n",
    "Now, let's look at the equation to update parameters for each layer in your network.\n",
    "\n",
    "$$\\large \\theta = \\theta - \\lambda\\cdot\\nabla_\\theta J(\\theta)$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6cb01a4-0c31-443e-85a4-ddcd4dd9215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, learning_rate):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        params: the dictornary to store all the layer parameters\n",
    "        learning_rate: the step length to update the parameters\n",
    "    Output: \n",
    "        params: the updated parameters\n",
    "    \"\"\"\n",
    "        \n",
    "    for layer_params in params.values():\n",
    "        if layer_params['w'].grad is not None: # check the gard is not None to not cause errors\n",
    "            layer_params['w'].data -= learning_rate * layer_params['w'].grad.data\n",
    "\n",
    "        if layer_params['b'].grad is not None:\n",
    "            layer_params['b'].data -= learning_rate * layer_params['b'].grad.data\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8bc51d9-e884-49e6-9f36-b451ed5b43b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grad(params):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        params: the dictornary to store all the layer parameters\n",
    "    Output: \n",
    "        params: the updated parameters with gradients clear\n",
    "    \"\"\"\n",
    "    #TODO: set the gradients with respect to parameters as zero\n",
    "\n",
    "    for layer, layer_params in params.items():\n",
    "        if layer_params['w'].grad is not None:\n",
    "            layer_params['w'].grad.data.zero_()\n",
    "        if layer_params['b'].grad is not None:\n",
    "            layer_params['b'].grad.data.zero_()\n",
    "    \n",
    "    return params\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba5bb37d-24ab-4b59-b1d3-3466f9b93293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(loss, params, learning_rate):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        loss: the loss tensor from the objective funtion that can be used to compute gradients\n",
    "        params: parameters of the four layers\n",
    "        learning_rate: the size of steps when updating parameters\n",
    "    Output:\n",
    "        params: parameters after one backpropogation\n",
    "    \"\"\"    \n",
    "    \n",
    "    # 1- claculate gradients for all relevent params (w,b) \n",
    "    loss.backward()\n",
    "\n",
    "    #2- update params after calculating grads \n",
    "    params = update_params(params, learning_rate)\n",
    "\n",
    "    #3- zero gradients so it does not affect future grad calculations \n",
    "    params = zero_grad(params) \n",
    "   \n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5f7c88-cc1f-4a57-bda6-dd40c12c8b03",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Training loop\n",
    "\n",
    "For this binary classification task, a standard objective function **Binary Cross-Entropy Loss** is used. Related detail is given as follows:\n",
    "\n",
    "$$\\large L = -\\frac{1}{N}\\sum_{i=1}^{N}( y_i \\cdot \\log(p(y_i))+(1-y_i)\\log(1-p(y_i)))$$\n",
    "\n",
    "where $y$ is the label (1 for dog and 0 for cat in our case) and $p(y)$ is the predicted probability, here $N$ equals to the batch_size.\n",
    "\n",
    "\n",
    "A initialization function is provided to help your network converge faster.\n",
    "\n",
    "```python\n",
    "def init_params(params):\n",
    "    \"\"\"\n",
    "    Initialize the parameters of each layer\n",
    "    \"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b4f2a3e-8207-4a80-9ec3-7fbdee62e743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lr(learning_rate, epoch):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        learning_rate: the input learning rate\n",
    "        epoch: which epoch you are in\n",
    "    Output:\n",
    "        learning_rate: the updated learning rate\n",
    "    \"\"\"    \n",
    "    \n",
    "    if (epoch + 1) % 15 == 0:\n",
    "        learning_rate *= 0.3\n",
    "        \n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc00259-4cf1-4afe-a4db-3bac70737632",
   "metadata": {},
   "source": [
    "**loss function**. Here is the helpful link: https://pytorch.org/docs/stable/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdffcc5d-5909-46f7-9dca-71e8df50dbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Step    10] loss: 0.656\n",
      "[Epoch 1, Step    20] loss: 0.668\n",
      "[Epoch 1, Step    30] loss: 0.576\n",
      "[Epoch 1, Step    40] loss: 0.553\n",
      "[Epoch 1, Step    50] loss: 0.541\n",
      "[Epoch 1, Step    60] loss: 0.432\n",
      "[Epoch 1, Step    70] loss: 0.510\n",
      "[Epoch 1, Step    80] loss: 0.482\n",
      "[Epoch 1, Step    90] loss: 0.534\n",
      "[Epoch 1, Step   100] loss: 0.560\n",
      "[Epoch 1, Step   110] loss: 0.540\n",
      "[Epoch 1, Step   120] loss: 0.479\n",
      "[Epoch 1, Step   130] loss: 0.523\n",
      "[Epoch 2, Step    10] loss: 0.492\n",
      "[Epoch 2, Step    20] loss: 0.465\n",
      "[Epoch 2, Step    30] loss: 0.452\n",
      "[Epoch 2, Step    40] loss: 0.500\n",
      "[Epoch 2, Step    50] loss: 0.456\n",
      "[Epoch 2, Step    60] loss: 0.520\n",
      "[Epoch 2, Step    70] loss: 0.510\n",
      "[Epoch 2, Step    80] loss: 0.470\n",
      "[Epoch 2, Step    90] loss: 0.475\n",
      "[Epoch 2, Step   100] loss: 0.445\n",
      "[Epoch 2, Step   110] loss: 0.468\n",
      "[Epoch 2, Step   120] loss: 0.450\n",
      "[Epoch 2, Step   130] loss: 0.445\n",
      "[Epoch 3, Step    10] loss: 0.457\n",
      "[Epoch 3, Step    20] loss: 0.454\n",
      "[Epoch 3, Step    30] loss: 0.482\n",
      "[Epoch 3, Step    40] loss: 0.469\n",
      "[Epoch 3, Step    50] loss: 0.441\n",
      "[Epoch 3, Step    60] loss: 0.430\n",
      "[Epoch 3, Step    70] loss: 0.471\n",
      "[Epoch 3, Step    80] loss: 0.437\n",
      "[Epoch 3, Step    90] loss: 0.420\n",
      "[Epoch 3, Step   100] loss: 0.462\n",
      "[Epoch 3, Step   110] loss: 0.465\n",
      "[Epoch 3, Step   120] loss: 0.431\n",
      "[Epoch 3, Step   130] loss: 0.469\n",
      "[Epoch 4, Step    10] loss: 0.445\n",
      "[Epoch 4, Step    20] loss: 0.470\n",
      "[Epoch 4, Step    30] loss: 0.427\n",
      "[Epoch 4, Step    40] loss: 0.384\n",
      "[Epoch 4, Step    50] loss: 0.450\n",
      "[Epoch 4, Step    60] loss: 0.432\n",
      "[Epoch 4, Step    70] loss: 0.540\n",
      "[Epoch 4, Step    80] loss: 0.442\n",
      "[Epoch 4, Step    90] loss: 0.448\n",
      "[Epoch 4, Step   100] loss: 0.457\n",
      "[Epoch 4, Step   110] loss: 0.470\n",
      "[Epoch 4, Step   120] loss: 0.424\n",
      "[Epoch 4, Step   130] loss: 0.412\n",
      "[Epoch 5, Step    10] loss: 0.417\n",
      "[Epoch 5, Step    20] loss: 0.441\n",
      "[Epoch 5, Step    30] loss: 0.410\n",
      "[Epoch 5, Step    40] loss: 0.349\n",
      "[Epoch 5, Step    50] loss: 0.461\n",
      "[Epoch 5, Step    60] loss: 0.437\n",
      "[Epoch 5, Step    70] loss: 0.430\n",
      "[Epoch 5, Step    80] loss: 0.438\n",
      "[Epoch 5, Step    90] loss: 0.420\n",
      "[Epoch 5, Step   100] loss: 0.470\n",
      "[Epoch 5, Step   110] loss: 0.413\n",
      "[Epoch 5, Step   120] loss: 0.453\n",
      "[Epoch 5, Step   130] loss: 0.477\n",
      "[Epoch 6, Step    10] loss: 0.352\n",
      "[Epoch 6, Step    20] loss: 0.389\n",
      "[Epoch 6, Step    30] loss: 0.437\n",
      "[Epoch 6, Step    40] loss: 0.461\n",
      "[Epoch 6, Step    50] loss: 0.378\n",
      "[Epoch 6, Step    60] loss: 0.422\n",
      "[Epoch 6, Step    70] loss: 0.436\n",
      "[Epoch 6, Step    80] loss: 0.441\n",
      "[Epoch 6, Step    90] loss: 0.468\n",
      "[Epoch 6, Step   100] loss: 0.379\n",
      "[Epoch 6, Step   110] loss: 0.453\n",
      "[Epoch 6, Step   120] loss: 0.441\n",
      "[Epoch 6, Step   130] loss: 0.411\n",
      "[Epoch 7, Step    10] loss: 0.392\n",
      "[Epoch 7, Step    20] loss: 0.401\n",
      "[Epoch 7, Step    30] loss: 0.411\n",
      "[Epoch 7, Step    40] loss: 0.407\n",
      "[Epoch 7, Step    50] loss: 0.415\n",
      "[Epoch 7, Step    60] loss: 0.438\n",
      "[Epoch 7, Step    70] loss: 0.508\n",
      "[Epoch 7, Step    80] loss: 0.398\n",
      "[Epoch 7, Step    90] loss: 0.398\n",
      "[Epoch 7, Step   100] loss: 0.404\n",
      "[Epoch 7, Step   110] loss: 0.460\n",
      "[Epoch 7, Step   120] loss: 0.366\n",
      "[Epoch 7, Step   130] loss: 0.425\n",
      "[Epoch 8, Step    10] loss: 0.410\n",
      "[Epoch 8, Step    20] loss: 0.377\n",
      "[Epoch 8, Step    30] loss: 0.428\n",
      "[Epoch 8, Step    40] loss: 0.419\n",
      "[Epoch 8, Step    50] loss: 0.458\n",
      "[Epoch 8, Step    60] loss: 0.404\n",
      "[Epoch 8, Step    70] loss: 0.382\n",
      "[Epoch 8, Step    80] loss: 0.387\n",
      "[Epoch 8, Step    90] loss: 0.417\n",
      "[Epoch 8, Step   100] loss: 0.423\n",
      "[Epoch 8, Step   110] loss: 0.400\n",
      "[Epoch 8, Step   120] loss: 0.432\n",
      "[Epoch 8, Step   130] loss: 0.437\n",
      "[Epoch 9, Step    10] loss: 0.391\n",
      "[Epoch 9, Step    20] loss: 0.412\n",
      "[Epoch 9, Step    30] loss: 0.366\n",
      "[Epoch 9, Step    40] loss: 0.382\n",
      "[Epoch 9, Step    50] loss: 0.425\n",
      "[Epoch 9, Step    60] loss: 0.450\n",
      "[Epoch 9, Step    70] loss: 0.385\n",
      "[Epoch 9, Step    80] loss: 0.401\n",
      "[Epoch 9, Step    90] loss: 0.410\n",
      "[Epoch 9, Step   100] loss: 0.378\n",
      "[Epoch 9, Step   110] loss: 0.487\n",
      "[Epoch 9, Step   120] loss: 0.455\n",
      "[Epoch 9, Step   130] loss: 0.467\n",
      "[Epoch 10, Step    10] loss: 0.345\n",
      "[Epoch 10, Step    20] loss: 0.360\n",
      "[Epoch 10, Step    30] loss: 0.408\n",
      "[Epoch 10, Step    40] loss: 0.429\n",
      "[Epoch 10, Step    50] loss: 0.438\n",
      "[Epoch 10, Step    60] loss: 0.369\n",
      "[Epoch 10, Step    70] loss: 0.386\n",
      "[Epoch 10, Step    80] loss: 0.383\n",
      "[Epoch 10, Step    90] loss: 0.432\n",
      "[Epoch 10, Step   100] loss: 0.436\n",
      "[Epoch 10, Step   110] loss: 0.391\n",
      "[Epoch 10, Step   120] loss: 0.414\n",
      "[Epoch 10, Step   130] loss: 0.380\n",
      "[Epoch 11, Step    10] loss: 0.354\n",
      "[Epoch 11, Step    20] loss: 0.403\n",
      "[Epoch 11, Step    30] loss: 0.389\n",
      "[Epoch 11, Step    40] loss: 0.397\n",
      "[Epoch 11, Step    50] loss: 0.366\n",
      "[Epoch 11, Step    60] loss: 0.382\n",
      "[Epoch 11, Step    70] loss: 0.494\n",
      "[Epoch 11, Step    80] loss: 0.419\n",
      "[Epoch 11, Step    90] loss: 0.329\n",
      "[Epoch 11, Step   100] loss: 0.386\n",
      "[Epoch 11, Step   110] loss: 0.391\n",
      "[Epoch 11, Step   120] loss: 0.412\n",
      "[Epoch 11, Step   130] loss: 0.359\n",
      "[Epoch 12, Step    10] loss: 0.412\n",
      "[Epoch 12, Step    20] loss: 0.372\n",
      "[Epoch 12, Step    30] loss: 0.397\n",
      "[Epoch 12, Step    40] loss: 0.385\n",
      "[Epoch 12, Step    50] loss: 0.376\n",
      "[Epoch 12, Step    60] loss: 0.350\n",
      "[Epoch 12, Step    70] loss: 0.354\n",
      "[Epoch 12, Step    80] loss: 0.418\n",
      "[Epoch 12, Step    90] loss: 0.387\n",
      "[Epoch 12, Step   100] loss: 0.420\n",
      "[Epoch 12, Step   110] loss: 0.407\n",
      "[Epoch 12, Step   120] loss: 0.359\n",
      "[Epoch 12, Step   130] loss: 0.423\n",
      "[Epoch 13, Step    10] loss: 0.398\n",
      "[Epoch 13, Step    20] loss: 0.384\n",
      "[Epoch 13, Step    30] loss: 0.342\n",
      "[Epoch 13, Step    40] loss: 0.409\n",
      "[Epoch 13, Step    50] loss: 0.350\n",
      "[Epoch 13, Step    60] loss: 0.403\n",
      "[Epoch 13, Step    70] loss: 0.353\n",
      "[Epoch 13, Step    80] loss: 0.344\n",
      "[Epoch 13, Step    90] loss: 0.362\n",
      "[Epoch 13, Step   100] loss: 0.381\n",
      "[Epoch 13, Step   110] loss: 0.423\n",
      "[Epoch 13, Step   120] loss: 0.397\n",
      "[Epoch 13, Step   130] loss: 0.377\n",
      "[Epoch 14, Step    10] loss: 0.385\n",
      "[Epoch 14, Step    20] loss: 0.447\n",
      "[Epoch 14, Step    30] loss: 0.375\n",
      "[Epoch 14, Step    40] loss: 0.359\n",
      "[Epoch 14, Step    50] loss: 0.374\n",
      "[Epoch 14, Step    60] loss: 0.387\n",
      "[Epoch 14, Step    70] loss: 0.371\n",
      "[Epoch 14, Step    80] loss: 0.346\n",
      "[Epoch 14, Step    90] loss: 0.398\n",
      "[Epoch 14, Step   100] loss: 0.367\n",
      "[Epoch 14, Step   110] loss: 0.399\n",
      "[Epoch 14, Step   120] loss: 0.420\n",
      "[Epoch 14, Step   130] loss: 0.410\n",
      "[Epoch 15, Step    10] loss: 0.363\n",
      "[Epoch 15, Step    20] loss: 0.351\n",
      "[Epoch 15, Step    30] loss: 0.358\n",
      "[Epoch 15, Step    40] loss: 0.418\n",
      "[Epoch 15, Step    50] loss: 0.371\n",
      "[Epoch 15, Step    60] loss: 0.365\n",
      "[Epoch 15, Step    70] loss: 0.349\n",
      "[Epoch 15, Step    80] loss: 0.386\n",
      "[Epoch 15, Step    90] loss: 0.372\n",
      "[Epoch 15, Step   100] loss: 0.427\n",
      "[Epoch 15, Step   110] loss: 0.479\n",
      "[Epoch 15, Step   120] loss: 0.372\n",
      "[Epoch 15, Step   130] loss: 0.346\n",
      "[Epoch 16, Step    10] loss: 0.368\n",
      "[Epoch 16, Step    20] loss: 0.376\n",
      "[Epoch 16, Step    30] loss: 0.377\n",
      "[Epoch 16, Step    40] loss: 0.344\n",
      "[Epoch 16, Step    50] loss: 0.341\n",
      "[Epoch 16, Step    60] loss: 0.356\n",
      "[Epoch 16, Step    70] loss: 0.348\n",
      "[Epoch 16, Step    80] loss: 0.382\n",
      "[Epoch 16, Step    90] loss: 0.338\n",
      "[Epoch 16, Step   100] loss: 0.390\n",
      "[Epoch 16, Step   110] loss: 0.366\n",
      "[Epoch 16, Step   120] loss: 0.369\n",
      "[Epoch 16, Step   130] loss: 0.403\n",
      "[Epoch 17, Step    10] loss: 0.371\n",
      "[Epoch 17, Step    20] loss: 0.362\n",
      "[Epoch 17, Step    30] loss: 0.385\n",
      "[Epoch 17, Step    40] loss: 0.369\n",
      "[Epoch 17, Step    50] loss: 0.345\n",
      "[Epoch 17, Step    60] loss: 0.388\n",
      "[Epoch 17, Step    70] loss: 0.332\n",
      "[Epoch 17, Step    80] loss: 0.358\n",
      "[Epoch 17, Step    90] loss: 0.347\n",
      "[Epoch 17, Step   100] loss: 0.351\n",
      "[Epoch 17, Step   110] loss: 0.355\n",
      "[Epoch 17, Step   120] loss: 0.359\n",
      "[Epoch 17, Step   130] loss: 0.341\n",
      "[Epoch 18, Step    10] loss: 0.377\n",
      "[Epoch 18, Step    20] loss: 0.348\n",
      "[Epoch 18, Step    30] loss: 0.336\n",
      "[Epoch 18, Step    40] loss: 0.356\n",
      "[Epoch 18, Step    50] loss: 0.414\n",
      "[Epoch 18, Step    60] loss: 0.359\n",
      "[Epoch 18, Step    70] loss: 0.348\n",
      "[Epoch 18, Step    80] loss: 0.339\n",
      "[Epoch 18, Step    90] loss: 0.364\n",
      "[Epoch 18, Step   100] loss: 0.346\n",
      "[Epoch 18, Step   110] loss: 0.348\n",
      "[Epoch 18, Step   120] loss: 0.327\n",
      "[Epoch 18, Step   130] loss: 0.374\n",
      "[Epoch 19, Step    10] loss: 0.387\n",
      "[Epoch 19, Step    20] loss: 0.385\n",
      "[Epoch 19, Step    30] loss: 0.332\n",
      "[Epoch 19, Step    40] loss: 0.356\n",
      "[Epoch 19, Step    50] loss: 0.354\n",
      "[Epoch 19, Step    60] loss: 0.347\n",
      "[Epoch 19, Step    70] loss: 0.382\n",
      "[Epoch 19, Step    80] loss: 0.336\n",
      "[Epoch 19, Step    90] loss: 0.343\n",
      "[Epoch 19, Step   100] loss: 0.352\n",
      "[Epoch 19, Step   110] loss: 0.351\n",
      "[Epoch 19, Step   120] loss: 0.354\n",
      "[Epoch 19, Step   130] loss: 0.326\n",
      "[Epoch 20, Step    10] loss: 0.332\n",
      "[Epoch 20, Step    20] loss: 0.379\n",
      "[Epoch 20, Step    30] loss: 0.357\n",
      "[Epoch 20, Step    40] loss: 0.355\n",
      "[Epoch 20, Step    50] loss: 0.341\n",
      "[Epoch 20, Step    60] loss: 0.348\n",
      "[Epoch 20, Step    70] loss: 0.385\n",
      "[Epoch 20, Step    80] loss: 0.342\n",
      "[Epoch 20, Step    90] loss: 0.366\n",
      "[Epoch 20, Step   100] loss: 0.358\n",
      "[Epoch 20, Step   110] loss: 0.327\n",
      "[Epoch 20, Step   120] loss: 0.358\n",
      "[Epoch 20, Step   130] loss: 0.349\n",
      "[Epoch 21, Step    10] loss: 0.366\n",
      "[Epoch 21, Step    20] loss: 0.347\n",
      "[Epoch 21, Step    30] loss: 0.327\n",
      "[Epoch 21, Step    40] loss: 0.325\n",
      "[Epoch 21, Step    50] loss: 0.339\n",
      "[Epoch 21, Step    60] loss: 0.379\n",
      "[Epoch 21, Step    70] loss: 0.386\n",
      "[Epoch 21, Step    80] loss: 0.390\n",
      "[Epoch 21, Step    90] loss: 0.349\n",
      "[Epoch 21, Step   100] loss: 0.344\n",
      "[Epoch 21, Step   110] loss: 0.356\n",
      "[Epoch 21, Step   120] loss: 0.325\n",
      "[Epoch 21, Step   130] loss: 0.345\n",
      "[Epoch 22, Step    10] loss: 0.350\n",
      "[Epoch 22, Step    20] loss: 0.326\n",
      "[Epoch 22, Step    30] loss: 0.387\n",
      "[Epoch 22, Step    40] loss: 0.333\n",
      "[Epoch 22, Step    50] loss: 0.354\n",
      "[Epoch 22, Step    60] loss: 0.336\n",
      "[Epoch 22, Step    70] loss: 0.360\n",
      "[Epoch 22, Step    80] loss: 0.327\n",
      "[Epoch 22, Step    90] loss: 0.345\n",
      "[Epoch 22, Step   100] loss: 0.358\n",
      "[Epoch 22, Step   110] loss: 0.358\n",
      "[Epoch 22, Step   120] loss: 0.359\n",
      "[Epoch 22, Step   130] loss: 0.358\n",
      "[Epoch 23, Step    10] loss: 0.336\n",
      "[Epoch 23, Step    20] loss: 0.348\n",
      "[Epoch 23, Step    30] loss: 0.345\n",
      "[Epoch 23, Step    40] loss: 0.368\n",
      "[Epoch 23, Step    50] loss: 0.340\n",
      "[Epoch 23, Step    60] loss: 0.355\n",
      "[Epoch 23, Step    70] loss: 0.363\n",
      "[Epoch 23, Step    80] loss: 0.319\n",
      "[Epoch 23, Step    90] loss: 0.371\n",
      "[Epoch 23, Step   100] loss: 0.328\n",
      "[Epoch 23, Step   110] loss: 0.350\n",
      "[Epoch 23, Step   120] loss: 0.330\n",
      "[Epoch 23, Step   130] loss: 0.403\n",
      "[Epoch 24, Step    10] loss: 0.350\n",
      "[Epoch 24, Step    20] loss: 0.352\n",
      "[Epoch 24, Step    30] loss: 0.334\n",
      "[Epoch 24, Step    40] loss: 0.418\n",
      "[Epoch 24, Step    50] loss: 0.351\n",
      "[Epoch 24, Step    60] loss: 0.331\n",
      "[Epoch 24, Step    70] loss: 0.327\n",
      "[Epoch 24, Step    80] loss: 0.329\n",
      "[Epoch 24, Step    90] loss: 0.347\n",
      "[Epoch 24, Step   100] loss: 0.355\n",
      "[Epoch 24, Step   110] loss: 0.335\n",
      "[Epoch 24, Step   120] loss: 0.335\n",
      "[Epoch 24, Step   130] loss: 0.351\n",
      "[Epoch 25, Step    10] loss: 0.379\n",
      "[Epoch 25, Step    20] loss: 0.354\n",
      "[Epoch 25, Step    30] loss: 0.359\n",
      "[Epoch 25, Step    40] loss: 0.333\n",
      "[Epoch 25, Step    50] loss: 0.363\n",
      "[Epoch 25, Step    60] loss: 0.343\n",
      "[Epoch 25, Step    70] loss: 0.365\n",
      "[Epoch 25, Step    80] loss: 0.344\n",
      "[Epoch 25, Step    90] loss: 0.342\n",
      "[Epoch 25, Step   100] loss: 0.337\n",
      "[Epoch 25, Step   110] loss: 0.325\n",
      "[Epoch 25, Step   120] loss: 0.331\n",
      "[Epoch 25, Step   130] loss: 0.320\n",
      "[Epoch 26, Step    10] loss: 0.346\n",
      "[Epoch 26, Step    20] loss: 0.362\n",
      "[Epoch 26, Step    30] loss: 0.332\n",
      "[Epoch 26, Step    40] loss: 0.357\n",
      "[Epoch 26, Step    50] loss: 0.337\n",
      "[Epoch 26, Step    60] loss: 0.338\n",
      "[Epoch 26, Step    70] loss: 0.353\n",
      "[Epoch 26, Step    80] loss: 0.355\n",
      "[Epoch 26, Step    90] loss: 0.347\n",
      "[Epoch 26, Step   100] loss: 0.340\n",
      "[Epoch 26, Step   110] loss: 0.326\n",
      "[Epoch 26, Step   120] loss: 0.346\n",
      "[Epoch 26, Step   130] loss: 0.344\n",
      "[Epoch 27, Step    10] loss: 0.339\n",
      "[Epoch 27, Step    20] loss: 0.332\n",
      "[Epoch 27, Step    30] loss: 0.318\n",
      "[Epoch 27, Step    40] loss: 0.332\n",
      "[Epoch 27, Step    50] loss: 0.383\n",
      "[Epoch 27, Step    60] loss: 0.356\n",
      "[Epoch 27, Step    70] loss: 0.358\n",
      "[Epoch 27, Step    80] loss: 0.353\n",
      "[Epoch 27, Step    90] loss: 0.344\n",
      "[Epoch 27, Step   100] loss: 0.338\n",
      "[Epoch 27, Step   110] loss: 0.342\n",
      "[Epoch 27, Step   120] loss: 0.336\n",
      "[Epoch 27, Step   130] loss: 0.327\n",
      "[Epoch 28, Step    10] loss: 0.339\n",
      "[Epoch 28, Step    20] loss: 0.373\n",
      "[Epoch 28, Step    30] loss: 0.337\n",
      "[Epoch 28, Step    40] loss: 0.324\n",
      "[Epoch 28, Step    50] loss: 0.319\n",
      "[Epoch 28, Step    60] loss: 0.352\n",
      "[Epoch 28, Step    70] loss: 0.356\n",
      "[Epoch 28, Step    80] loss: 0.373\n",
      "[Epoch 28, Step    90] loss: 0.349\n",
      "[Epoch 28, Step   100] loss: 0.339\n",
      "[Epoch 28, Step   110] loss: 0.322\n",
      "[Epoch 28, Step   120] loss: 0.329\n",
      "[Epoch 28, Step   130] loss: 0.330\n",
      "[Epoch 29, Step    10] loss: 0.353\n",
      "[Epoch 29, Step    20] loss: 0.320\n",
      "[Epoch 29, Step    30] loss: 0.341\n",
      "[Epoch 29, Step    40] loss: 0.345\n",
      "[Epoch 29, Step    50] loss: 0.324\n",
      "[Epoch 29, Step    60] loss: 0.344\n",
      "[Epoch 29, Step    70] loss: 0.323\n",
      "[Epoch 29, Step    80] loss: 0.348\n",
      "[Epoch 29, Step    90] loss: 0.363\n",
      "[Epoch 29, Step   100] loss: 0.358\n",
      "[Epoch 29, Step   110] loss: 0.329\n",
      "[Epoch 29, Step   120] loss: 0.352\n",
      "[Epoch 29, Step   130] loss: 0.341\n",
      "[Epoch 30, Step    10] loss: 0.320\n",
      "[Epoch 30, Step    20] loss: 0.362\n",
      "[Epoch 30, Step    30] loss: 0.333\n",
      "[Epoch 30, Step    40] loss: 0.344\n",
      "[Epoch 30, Step    50] loss: 0.326\n",
      "[Epoch 30, Step    60] loss: 0.333\n",
      "[Epoch 30, Step    70] loss: 0.334\n",
      "[Epoch 30, Step    80] loss: 0.346\n",
      "[Epoch 30, Step    90] loss: 0.352\n",
      "[Epoch 30, Step   100] loss: 0.354\n",
      "[Epoch 30, Step   110] loss: 0.357\n",
      "[Epoch 30, Step   120] loss: 0.322\n",
      "[Epoch 30, Step   130] loss: 0.338\n",
      "[Epoch 31, Step    10] loss: 0.345\n",
      "[Epoch 31, Step    20] loss: 0.334\n",
      "[Epoch 31, Step    30] loss: 0.346\n",
      "[Epoch 31, Step    40] loss: 0.332\n",
      "[Epoch 31, Step    50] loss: 0.357\n",
      "[Epoch 31, Step    60] loss: 0.350\n",
      "[Epoch 31, Step    70] loss: 0.320\n",
      "[Epoch 31, Step    80] loss: 0.326\n",
      "[Epoch 31, Step    90] loss: 0.333\n",
      "[Epoch 31, Step   100] loss: 0.331\n",
      "[Epoch 31, Step   110] loss: 0.343\n",
      "[Epoch 31, Step   120] loss: 0.336\n",
      "[Epoch 31, Step   130] loss: 0.351\n",
      "[Epoch 32, Step    10] loss: 0.334\n",
      "[Epoch 32, Step    20] loss: 0.344\n",
      "[Epoch 32, Step    30] loss: 0.319\n",
      "[Epoch 32, Step    40] loss: 0.339\n",
      "[Epoch 32, Step    50] loss: 0.358\n",
      "[Epoch 32, Step    60] loss: 0.346\n",
      "[Epoch 32, Step    70] loss: 0.336\n",
      "[Epoch 32, Step    80] loss: 0.334\n",
      "[Epoch 32, Step    90] loss: 0.353\n",
      "[Epoch 32, Step   100] loss: 0.335\n",
      "[Epoch 32, Step   110] loss: 0.331\n",
      "[Epoch 32, Step   120] loss: 0.348\n",
      "[Epoch 32, Step   130] loss: 0.322\n",
      "[Epoch 33, Step    10] loss: 0.354\n",
      "[Epoch 33, Step    20] loss: 0.360\n",
      "[Epoch 33, Step    30] loss: 0.333\n",
      "[Epoch 33, Step    40] loss: 0.324\n",
      "[Epoch 33, Step    50] loss: 0.332\n",
      "[Epoch 33, Step    60] loss: 0.320\n",
      "[Epoch 33, Step    70] loss: 0.320\n",
      "[Epoch 33, Step    80] loss: 0.345\n",
      "[Epoch 33, Step    90] loss: 0.327\n",
      "[Epoch 33, Step   100] loss: 0.348\n",
      "[Epoch 33, Step   110] loss: 0.360\n",
      "[Epoch 33, Step   120] loss: 0.334\n",
      "[Epoch 33, Step   130] loss: 0.345\n",
      "[Epoch 34, Step    10] loss: 0.336\n",
      "[Epoch 34, Step    20] loss: 0.352\n",
      "[Epoch 34, Step    30] loss: 0.371\n",
      "[Epoch 34, Step    40] loss: 0.319\n",
      "[Epoch 34, Step    50] loss: 0.333\n",
      "[Epoch 34, Step    60] loss: 0.321\n",
      "[Epoch 34, Step    70] loss: 0.319\n",
      "[Epoch 34, Step    80] loss: 0.344\n",
      "[Epoch 34, Step    90] loss: 0.339\n",
      "[Epoch 34, Step   100] loss: 0.345\n",
      "[Epoch 34, Step   110] loss: 0.360\n",
      "[Epoch 34, Step   120] loss: 0.341\n",
      "[Epoch 34, Step   130] loss: 0.316\n",
      "[Epoch 35, Step    10] loss: 0.345\n",
      "[Epoch 35, Step    20] loss: 0.332\n",
      "[Epoch 35, Step    30] loss: 0.332\n",
      "[Epoch 35, Step    40] loss: 0.346\n",
      "[Epoch 35, Step    50] loss: 0.332\n",
      "[Epoch 35, Step    60] loss: 0.352\n",
      "[Epoch 35, Step    70] loss: 0.369\n",
      "[Epoch 35, Step    80] loss: 0.346\n",
      "[Epoch 35, Step    90] loss: 0.344\n",
      "[Epoch 35, Step   100] loss: 0.318\n",
      "[Epoch 35, Step   110] loss: 0.320\n",
      "[Epoch 35, Step   120] loss: 0.324\n",
      "[Epoch 35, Step   130] loss: 0.329\n",
      "[Epoch 36, Step    10] loss: 0.339\n",
      "[Epoch 36, Step    20] loss: 0.346\n",
      "[Epoch 36, Step    30] loss: 0.362\n",
      "[Epoch 36, Step    40] loss: 0.335\n",
      "[Epoch 36, Step    50] loss: 0.336\n",
      "[Epoch 36, Step    60] loss: 0.320\n",
      "[Epoch 36, Step    70] loss: 0.320\n",
      "[Epoch 36, Step    80] loss: 0.330\n",
      "[Epoch 36, Step    90] loss: 0.347\n",
      "[Epoch 36, Step   100] loss: 0.321\n",
      "[Epoch 36, Step   110] loss: 0.330\n",
      "[Epoch 36, Step   120] loss: 0.343\n",
      "[Epoch 36, Step   130] loss: 0.359\n",
      "[Epoch 37, Step    10] loss: 0.359\n",
      "[Epoch 37, Step    20] loss: 0.318\n",
      "[Epoch 37, Step    30] loss: 0.323\n",
      "[Epoch 37, Step    40] loss: 0.325\n",
      "[Epoch 37, Step    50] loss: 0.331\n",
      "[Epoch 37, Step    60] loss: 0.355\n",
      "[Epoch 37, Step    70] loss: 0.333\n",
      "[Epoch 37, Step    80] loss: 0.332\n",
      "[Epoch 37, Step    90] loss: 0.345\n",
      "[Epoch 37, Step   100] loss: 0.324\n",
      "[Epoch 37, Step   110] loss: 0.346\n",
      "[Epoch 37, Step   120] loss: 0.357\n",
      "[Epoch 37, Step   130] loss: 0.333\n",
      "[Epoch 38, Step    10] loss: 0.320\n",
      "[Epoch 38, Step    20] loss: 0.326\n",
      "[Epoch 38, Step    30] loss: 0.332\n",
      "[Epoch 38, Step    40] loss: 0.342\n",
      "[Epoch 38, Step    50] loss: 0.335\n",
      "[Epoch 38, Step    60] loss: 0.331\n",
      "[Epoch 38, Step    70] loss: 0.321\n",
      "[Epoch 38, Step    80] loss: 0.337\n",
      "[Epoch 38, Step    90] loss: 0.343\n",
      "[Epoch 38, Step   100] loss: 0.357\n",
      "[Epoch 38, Step   110] loss: 0.333\n",
      "[Epoch 38, Step   120] loss: 0.369\n",
      "[Epoch 38, Step   130] loss: 0.331\n",
      "[Epoch 39, Step    10] loss: 0.318\n",
      "[Epoch 39, Step    20] loss: 0.349\n",
      "[Epoch 39, Step    30] loss: 0.324\n",
      "[Epoch 39, Step    40] loss: 0.336\n",
      "[Epoch 39, Step    50] loss: 0.330\n",
      "[Epoch 39, Step    60] loss: 0.319\n",
      "[Epoch 39, Step    70] loss: 0.359\n",
      "[Epoch 39, Step    80] loss: 0.331\n",
      "[Epoch 39, Step    90] loss: 0.355\n",
      "[Epoch 39, Step   100] loss: 0.344\n",
      "[Epoch 39, Step   110] loss: 0.370\n",
      "[Epoch 39, Step   120] loss: 0.322\n",
      "[Epoch 39, Step   130] loss: 0.319\n",
      "[Epoch 40, Step    10] loss: 0.331\n",
      "[Epoch 40, Step    20] loss: 0.334\n",
      "[Epoch 40, Step    30] loss: 0.362\n",
      "[Epoch 40, Step    40] loss: 0.331\n",
      "[Epoch 40, Step    50] loss: 0.332\n",
      "[Epoch 40, Step    60] loss: 0.336\n",
      "[Epoch 40, Step    70] loss: 0.347\n",
      "[Epoch 40, Step    80] loss: 0.334\n",
      "[Epoch 40, Step    90] loss: 0.319\n",
      "[Epoch 40, Step   100] loss: 0.355\n",
      "[Epoch 40, Step   110] loss: 0.344\n",
      "[Epoch 40, Step   120] loss: 0.329\n",
      "[Epoch 40, Step   130] loss: 0.319\n",
      "[Epoch 41, Step    10] loss: 0.320\n",
      "[Epoch 41, Step    20] loss: 0.356\n",
      "[Epoch 41, Step    30] loss: 0.318\n",
      "[Epoch 41, Step    40] loss: 0.342\n",
      "[Epoch 41, Step    50] loss: 0.337\n",
      "[Epoch 41, Step    60] loss: 0.361\n",
      "[Epoch 41, Step    70] loss: 0.330\n",
      "[Epoch 41, Step    80] loss: 0.331\n",
      "[Epoch 41, Step    90] loss: 0.320\n",
      "[Epoch 41, Step   100] loss: 0.335\n",
      "[Epoch 41, Step   110] loss: 0.330\n",
      "[Epoch 41, Step   120] loss: 0.344\n",
      "[Epoch 41, Step   130] loss: 0.347\n",
      "[Epoch 42, Step    10] loss: 0.343\n",
      "[Epoch 42, Step    20] loss: 0.343\n",
      "[Epoch 42, Step    30] loss: 0.343\n",
      "[Epoch 42, Step    40] loss: 0.329\n",
      "[Epoch 42, Step    50] loss: 0.322\n",
      "[Epoch 42, Step    60] loss: 0.348\n",
      "[Epoch 42, Step    70] loss: 0.343\n",
      "[Epoch 42, Step    80] loss: 0.342\n",
      "[Epoch 42, Step    90] loss: 0.318\n",
      "[Epoch 42, Step   100] loss: 0.342\n",
      "[Epoch 42, Step   110] loss: 0.333\n",
      "[Epoch 42, Step   120] loss: 0.339\n",
      "[Epoch 42, Step   130] loss: 0.324\n",
      "[Epoch 43, Step    10] loss: 0.330\n",
      "[Epoch 43, Step    20] loss: 0.319\n",
      "[Epoch 43, Step    30] loss: 0.359\n",
      "[Epoch 43, Step    40] loss: 0.321\n",
      "[Epoch 43, Step    50] loss: 0.317\n",
      "[Epoch 43, Step    60] loss: 0.368\n",
      "[Epoch 43, Step    70] loss: 0.322\n",
      "[Epoch 43, Step    80] loss: 0.335\n",
      "[Epoch 43, Step    90] loss: 0.330\n",
      "[Epoch 43, Step   100] loss: 0.343\n",
      "[Epoch 43, Step   110] loss: 0.321\n",
      "[Epoch 43, Step   120] loss: 0.358\n",
      "[Epoch 43, Step   130] loss: 0.346\n",
      "[Epoch 44, Step    10] loss: 0.333\n",
      "[Epoch 44, Step    20] loss: 0.344\n",
      "[Epoch 44, Step    30] loss: 0.333\n",
      "[Epoch 44, Step    40] loss: 0.342\n",
      "[Epoch 44, Step    50] loss: 0.321\n",
      "[Epoch 44, Step    60] loss: 0.335\n",
      "[Epoch 44, Step    70] loss: 0.330\n",
      "[Epoch 44, Step    80] loss: 0.331\n",
      "[Epoch 44, Step    90] loss: 0.355\n",
      "[Epoch 44, Step   100] loss: 0.331\n",
      "[Epoch 44, Step   110] loss: 0.346\n",
      "[Epoch 44, Step   120] loss: 0.347\n",
      "[Epoch 44, Step   130] loss: 0.321\n",
      "[Epoch 45, Step    10] loss: 0.369\n",
      "[Epoch 45, Step    20] loss: 0.319\n",
      "[Epoch 45, Step    30] loss: 0.318\n",
      "[Epoch 45, Step    40] loss: 0.332\n",
      "[Epoch 45, Step    50] loss: 0.332\n",
      "[Epoch 45, Step    60] loss: 0.341\n",
      "[Epoch 45, Step    70] loss: 0.339\n",
      "[Epoch 45, Step    80] loss: 0.321\n",
      "[Epoch 45, Step    90] loss: 0.317\n",
      "[Epoch 45, Step   100] loss: 0.356\n",
      "[Epoch 45, Step   110] loss: 0.333\n",
      "[Epoch 45, Step   120] loss: 0.344\n",
      "[Epoch 45, Step   130] loss: 0.345\n",
      "[Epoch 46, Step    10] loss: 0.333\n",
      "[Epoch 46, Step    20] loss: 0.344\n",
      "[Epoch 46, Step    30] loss: 0.335\n",
      "[Epoch 46, Step    40] loss: 0.324\n",
      "[Epoch 46, Step    50] loss: 0.342\n",
      "[Epoch 46, Step    60] loss: 0.332\n",
      "[Epoch 46, Step    70] loss: 0.343\n",
      "[Epoch 46, Step    80] loss: 0.342\n",
      "[Epoch 46, Step    90] loss: 0.319\n",
      "[Epoch 46, Step   100] loss: 0.320\n",
      "[Epoch 46, Step   110] loss: 0.331\n",
      "[Epoch 46, Step   120] loss: 0.345\n",
      "[Epoch 46, Step   130] loss: 0.355\n",
      "[Epoch 47, Step    10] loss: 0.332\n",
      "[Epoch 47, Step    20] loss: 0.330\n",
      "[Epoch 47, Step    30] loss: 0.333\n",
      "[Epoch 47, Step    40] loss: 0.331\n",
      "[Epoch 47, Step    50] loss: 0.320\n",
      "[Epoch 47, Step    60] loss: 0.333\n",
      "[Epoch 47, Step    70] loss: 0.330\n",
      "[Epoch 47, Step    80] loss: 0.333\n",
      "[Epoch 47, Step    90] loss: 0.343\n",
      "[Epoch 47, Step   100] loss: 0.358\n",
      "[Epoch 47, Step   110] loss: 0.367\n",
      "[Epoch 47, Step   120] loss: 0.338\n",
      "[Epoch 47, Step   130] loss: 0.316\n",
      "[Epoch 48, Step    10] loss: 0.317\n",
      "[Epoch 48, Step    20] loss: 0.318\n",
      "[Epoch 48, Step    30] loss: 0.342\n",
      "[Epoch 48, Step    40] loss: 0.317\n",
      "[Epoch 48, Step    50] loss: 0.343\n",
      "[Epoch 48, Step    60] loss: 0.332\n",
      "[Epoch 48, Step    70] loss: 0.335\n",
      "[Epoch 48, Step    80] loss: 0.321\n",
      "[Epoch 48, Step    90] loss: 0.372\n",
      "[Epoch 48, Step   100] loss: 0.322\n",
      "[Epoch 48, Step   110] loss: 0.345\n",
      "[Epoch 48, Step   120] loss: 0.343\n",
      "[Epoch 48, Step   130] loss: 0.355\n",
      "[Epoch 49, Step    10] loss: 0.319\n",
      "[Epoch 49, Step    20] loss: 0.334\n",
      "[Epoch 49, Step    30] loss: 0.318\n",
      "[Epoch 49, Step    40] loss: 0.334\n",
      "[Epoch 49, Step    50] loss: 0.344\n",
      "[Epoch 49, Step    60] loss: 0.343\n",
      "[Epoch 49, Step    70] loss: 0.321\n",
      "[Epoch 49, Step    80] loss: 0.355\n",
      "[Epoch 49, Step    90] loss: 0.344\n",
      "[Epoch 49, Step   100] loss: 0.337\n",
      "[Epoch 49, Step   110] loss: 0.317\n",
      "[Epoch 49, Step   120] loss: 0.366\n",
      "[Epoch 49, Step   130] loss: 0.332\n",
      "[Epoch 50, Step    10] loss: 0.332\n",
      "[Epoch 50, Step    20] loss: 0.318\n",
      "[Epoch 50, Step    30] loss: 0.325\n",
      "[Epoch 50, Step    40] loss: 0.343\n",
      "[Epoch 50, Step    50] loss: 0.355\n",
      "[Epoch 50, Step    60] loss: 0.331\n",
      "[Epoch 50, Step    70] loss: 0.344\n",
      "[Epoch 50, Step    80] loss: 0.343\n",
      "[Epoch 50, Step    90] loss: 0.320\n",
      "[Epoch 50, Step   100] loss: 0.383\n",
      "[Epoch 50, Step   110] loss: 0.320\n",
      "[Epoch 50, Step   120] loss: 0.329\n",
      "[Epoch 50, Step   130] loss: 0.319\n",
      "[Epoch 51, Step    10] loss: 0.332\n",
      "[Epoch 51, Step    20] loss: 0.345\n",
      "[Epoch 51, Step    30] loss: 0.346\n",
      "[Epoch 51, Step    40] loss: 0.319\n",
      "[Epoch 51, Step    50] loss: 0.343\n",
      "[Epoch 51, Step    60] loss: 0.330\n",
      "[Epoch 51, Step    70] loss: 0.330\n",
      "[Epoch 51, Step    80] loss: 0.318\n",
      "[Epoch 51, Step    90] loss: 0.330\n",
      "[Epoch 51, Step   100] loss: 0.331\n",
      "[Epoch 51, Step   110] loss: 0.344\n",
      "[Epoch 51, Step   120] loss: 0.357\n",
      "[Epoch 51, Step   130] loss: 0.337\n",
      "[Epoch 52, Step    10] loss: 0.342\n",
      "[Epoch 52, Step    20] loss: 0.319\n",
      "[Epoch 52, Step    30] loss: 0.332\n",
      "[Epoch 52, Step    40] loss: 0.342\n",
      "[Epoch 52, Step    50] loss: 0.332\n",
      "[Epoch 52, Step    60] loss: 0.338\n",
      "[Epoch 52, Step    70] loss: 0.379\n",
      "[Epoch 52, Step    80] loss: 0.334\n",
      "[Epoch 52, Step    90] loss: 0.329\n",
      "[Epoch 52, Step   100] loss: 0.344\n",
      "[Epoch 52, Step   110] loss: 0.321\n",
      "[Epoch 52, Step   120] loss: 0.332\n",
      "[Epoch 52, Step   130] loss: 0.318\n",
      "[Epoch 53, Step    10] loss: 0.344\n",
      "[Epoch 53, Step    20] loss: 0.319\n",
      "[Epoch 53, Step    30] loss: 0.342\n",
      "[Epoch 53, Step    40] loss: 0.346\n",
      "[Epoch 53, Step    50] loss: 0.346\n",
      "[Epoch 53, Step    60] loss: 0.343\n",
      "[Epoch 53, Step    70] loss: 0.333\n",
      "[Epoch 53, Step    80] loss: 0.342\n",
      "[Epoch 53, Step    90] loss: 0.330\n",
      "[Epoch 53, Step   100] loss: 0.318\n",
      "[Epoch 53, Step   110] loss: 0.345\n",
      "[Epoch 53, Step   120] loss: 0.332\n",
      "[Epoch 53, Step   130] loss: 0.321\n",
      "[Epoch 54, Step    10] loss: 0.321\n",
      "[Epoch 54, Step    20] loss: 0.317\n",
      "[Epoch 54, Step    30] loss: 0.331\n",
      "[Epoch 54, Step    40] loss: 0.346\n",
      "[Epoch 54, Step    50] loss: 0.323\n",
      "[Epoch 54, Step    60] loss: 0.343\n",
      "[Epoch 54, Step    70] loss: 0.331\n",
      "[Epoch 54, Step    80] loss: 0.330\n",
      "[Epoch 54, Step    90] loss: 0.359\n",
      "[Epoch 54, Step   100] loss: 0.318\n",
      "[Epoch 54, Step   110] loss: 0.318\n",
      "[Epoch 54, Step   120] loss: 0.355\n",
      "[Epoch 54, Step   130] loss: 0.368\n",
      "[Epoch 55, Step    10] loss: 0.323\n",
      "[Epoch 55, Step    20] loss: 0.368\n",
      "[Epoch 55, Step    30] loss: 0.319\n",
      "[Epoch 55, Step    40] loss: 0.354\n",
      "[Epoch 55, Step    50] loss: 0.331\n",
      "[Epoch 55, Step    60] loss: 0.342\n",
      "[Epoch 55, Step    70] loss: 0.333\n",
      "[Epoch 55, Step    80] loss: 0.331\n",
      "[Epoch 55, Step    90] loss: 0.344\n",
      "[Epoch 55, Step   100] loss: 0.319\n",
      "[Epoch 55, Step   110] loss: 0.329\n",
      "[Epoch 55, Step   120] loss: 0.329\n",
      "[Epoch 55, Step   130] loss: 0.336\n",
      "[Epoch 56, Step    10] loss: 0.337\n",
      "[Epoch 56, Step    20] loss: 0.317\n",
      "[Epoch 56, Step    30] loss: 0.330\n",
      "[Epoch 56, Step    40] loss: 0.357\n",
      "[Epoch 56, Step    50] loss: 0.318\n",
      "[Epoch 56, Step    60] loss: 0.320\n",
      "[Epoch 56, Step    70] loss: 0.331\n",
      "[Epoch 56, Step    80] loss: 0.366\n",
      "[Epoch 56, Step    90] loss: 0.318\n",
      "[Epoch 56, Step   100] loss: 0.344\n",
      "[Epoch 56, Step   110] loss: 0.336\n",
      "[Epoch 56, Step   120] loss: 0.330\n",
      "[Epoch 56, Step   130] loss: 0.355\n",
      "[Epoch 57, Step    10] loss: 0.342\n",
      "[Epoch 57, Step    20] loss: 0.322\n",
      "[Epoch 57, Step    30] loss: 0.342\n",
      "[Epoch 57, Step    40] loss: 0.331\n",
      "[Epoch 57, Step    50] loss: 0.330\n",
      "[Epoch 57, Step    60] loss: 0.318\n",
      "[Epoch 57, Step    70] loss: 0.344\n",
      "[Epoch 57, Step    80] loss: 0.368\n",
      "[Epoch 57, Step    90] loss: 0.319\n",
      "[Epoch 57, Step   100] loss: 0.344\n",
      "[Epoch 57, Step   110] loss: 0.333\n",
      "[Epoch 57, Step   120] loss: 0.330\n",
      "[Epoch 57, Step   130] loss: 0.335\n",
      "[Epoch 58, Step    10] loss: 0.318\n",
      "[Epoch 58, Step    20] loss: 0.329\n",
      "[Epoch 58, Step    30] loss: 0.317\n",
      "[Epoch 58, Step    40] loss: 0.356\n",
      "[Epoch 58, Step    50] loss: 0.345\n",
      "[Epoch 58, Step    60] loss: 0.320\n",
      "[Epoch 58, Step    70] loss: 0.318\n",
      "[Epoch 58, Step    80] loss: 0.368\n",
      "[Epoch 58, Step    90] loss: 0.331\n",
      "[Epoch 58, Step   100] loss: 0.331\n",
      "[Epoch 58, Step   110] loss: 0.356\n",
      "[Epoch 58, Step   120] loss: 0.333\n",
      "[Epoch 58, Step   130] loss: 0.335\n",
      "[Epoch 59, Step    10] loss: 0.318\n",
      "[Epoch 59, Step    20] loss: 0.331\n",
      "[Epoch 59, Step    30] loss: 0.334\n",
      "[Epoch 59, Step    40] loss: 0.320\n",
      "[Epoch 59, Step    50] loss: 0.347\n",
      "[Epoch 59, Step    60] loss: 0.319\n",
      "[Epoch 59, Step    70] loss: 0.369\n",
      "[Epoch 59, Step    80] loss: 0.330\n",
      "[Epoch 59, Step    90] loss: 0.330\n",
      "[Epoch 59, Step   100] loss: 0.342\n",
      "[Epoch 59, Step   110] loss: 0.358\n",
      "[Epoch 59, Step   120] loss: 0.330\n",
      "[Epoch 59, Step   130] loss: 0.330\n",
      "[Epoch 60, Step    10] loss: 0.343\n",
      "[Epoch 60, Step    20] loss: 0.321\n",
      "[Epoch 60, Step    30] loss: 0.341\n",
      "[Epoch 60, Step    40] loss: 0.342\n",
      "[Epoch 60, Step    50] loss: 0.357\n",
      "[Epoch 60, Step    60] loss: 0.319\n",
      "[Epoch 60, Step    70] loss: 0.342\n",
      "[Epoch 60, Step    80] loss: 0.330\n",
      "[Epoch 60, Step    90] loss: 0.319\n",
      "[Epoch 60, Step   100] loss: 0.321\n",
      "[Epoch 60, Step   110] loss: 0.331\n",
      "[Epoch 60, Step   120] loss: 0.356\n",
      "[Epoch 60, Step   130] loss: 0.334\n",
      "[Epoch 61, Step    10] loss: 0.321\n",
      "[Epoch 61, Step    20] loss: 0.342\n",
      "[Epoch 61, Step    30] loss: 0.356\n",
      "[Epoch 61, Step    40] loss: 0.334\n",
      "[Epoch 61, Step    50] loss: 0.382\n",
      "[Epoch 61, Step    60] loss: 0.318\n",
      "[Epoch 61, Step    70] loss: 0.332\n",
      "[Epoch 61, Step    80] loss: 0.317\n",
      "[Epoch 61, Step    90] loss: 0.343\n",
      "[Epoch 61, Step   100] loss: 0.332\n",
      "[Epoch 61, Step   110] loss: 0.330\n",
      "[Epoch 61, Step   120] loss: 0.329\n",
      "[Epoch 61, Step   130] loss: 0.322\n",
      "[Epoch 62, Step    10] loss: 0.358\n",
      "[Epoch 62, Step    20] loss: 0.343\n",
      "[Epoch 62, Step    30] loss: 0.331\n",
      "[Epoch 62, Step    40] loss: 0.344\n",
      "[Epoch 62, Step    50] loss: 0.333\n",
      "[Epoch 62, Step    60] loss: 0.328\n",
      "[Epoch 62, Step    70] loss: 0.317\n",
      "[Epoch 62, Step    80] loss: 0.356\n",
      "[Epoch 62, Step    90] loss: 0.317\n",
      "[Epoch 62, Step   100] loss: 0.318\n",
      "[Epoch 62, Step   110] loss: 0.346\n",
      "[Epoch 62, Step   120] loss: 0.322\n",
      "[Epoch 62, Step   130] loss: 0.343\n",
      "[Epoch 63, Step    10] loss: 0.330\n",
      "[Epoch 63, Step    20] loss: 0.332\n",
      "[Epoch 63, Step    30] loss: 0.366\n",
      "[Epoch 63, Step    40] loss: 0.331\n",
      "[Epoch 63, Step    50] loss: 0.318\n",
      "[Epoch 63, Step    60] loss: 0.358\n",
      "[Epoch 63, Step    70] loss: 0.333\n",
      "[Epoch 63, Step    80] loss: 0.321\n",
      "[Epoch 63, Step    90] loss: 0.369\n",
      "[Epoch 63, Step   100] loss: 0.318\n",
      "[Epoch 63, Step   110] loss: 0.330\n",
      "[Epoch 63, Step   120] loss: 0.329\n",
      "[Epoch 63, Step   130] loss: 0.323\n",
      "[Epoch 64, Step    10] loss: 0.332\n",
      "[Epoch 64, Step    20] loss: 0.343\n",
      "[Epoch 64, Step    30] loss: 0.344\n",
      "[Epoch 64, Step    40] loss: 0.332\n",
      "[Epoch 64, Step    50] loss: 0.320\n",
      "[Epoch 64, Step    60] loss: 0.344\n",
      "[Epoch 64, Step    70] loss: 0.331\n",
      "[Epoch 64, Step    80] loss: 0.319\n",
      "[Epoch 64, Step    90] loss: 0.316\n",
      "[Epoch 64, Step   100] loss: 0.344\n",
      "[Epoch 64, Step   110] loss: 0.358\n",
      "[Epoch 64, Step   120] loss: 0.330\n",
      "[Epoch 64, Step   130] loss: 0.343\n",
      "[Epoch 65, Step    10] loss: 0.334\n",
      "[Epoch 65, Step    20] loss: 0.319\n",
      "[Epoch 65, Step    30] loss: 0.319\n",
      "[Epoch 65, Step    40] loss: 0.330\n",
      "[Epoch 65, Step    50] loss: 0.355\n",
      "[Epoch 65, Step    60] loss: 0.355\n",
      "[Epoch 65, Step    70] loss: 0.345\n",
      "[Epoch 65, Step    80] loss: 0.330\n",
      "[Epoch 65, Step    90] loss: 0.330\n",
      "[Epoch 65, Step   100] loss: 0.346\n",
      "[Epoch 65, Step   110] loss: 0.318\n",
      "[Epoch 65, Step   120] loss: 0.330\n",
      "[Epoch 65, Step   130] loss: 0.344\n",
      "[Epoch 66, Step    10] loss: 0.343\n",
      "[Epoch 66, Step    20] loss: 0.317\n",
      "[Epoch 66, Step    30] loss: 0.332\n",
      "[Epoch 66, Step    40] loss: 0.380\n",
      "[Epoch 66, Step    50] loss: 0.333\n",
      "[Epoch 66, Step    60] loss: 0.344\n",
      "[Epoch 66, Step    70] loss: 0.331\n",
      "[Epoch 66, Step    80] loss: 0.331\n",
      "[Epoch 66, Step    90] loss: 0.331\n",
      "[Epoch 66, Step   100] loss: 0.319\n",
      "[Epoch 66, Step   110] loss: 0.329\n",
      "[Epoch 66, Step   120] loss: 0.330\n",
      "[Epoch 66, Step   130] loss: 0.337\n",
      "[Epoch 67, Step    10] loss: 0.343\n",
      "[Epoch 67, Step    20] loss: 0.330\n",
      "[Epoch 67, Step    30] loss: 0.333\n",
      "[Epoch 67, Step    40] loss: 0.342\n",
      "[Epoch 67, Step    50] loss: 0.343\n",
      "[Epoch 67, Step    60] loss: 0.318\n",
      "[Epoch 67, Step    70] loss: 0.344\n",
      "[Epoch 67, Step    80] loss: 0.325\n",
      "[Epoch 67, Step    90] loss: 0.320\n",
      "[Epoch 67, Step   100] loss: 0.319\n",
      "[Epoch 67, Step   110] loss: 0.341\n",
      "[Epoch 67, Step   120] loss: 0.354\n",
      "[Epoch 67, Step   130] loss: 0.343\n",
      "[Epoch 68, Step    10] loss: 0.355\n",
      "[Epoch 68, Step    20] loss: 0.319\n",
      "[Epoch 68, Step    30] loss: 0.330\n",
      "[Epoch 68, Step    40] loss: 0.320\n",
      "[Epoch 68, Step    50] loss: 0.332\n",
      "[Epoch 68, Step    60] loss: 0.362\n",
      "[Epoch 68, Step    70] loss: 0.366\n",
      "[Epoch 68, Step    80] loss: 0.321\n",
      "[Epoch 68, Step    90] loss: 0.330\n",
      "[Epoch 68, Step   100] loss: 0.329\n",
      "[Epoch 68, Step   110] loss: 0.318\n",
      "[Epoch 68, Step   120] loss: 0.342\n",
      "[Epoch 68, Step   130] loss: 0.331\n",
      "[Epoch 69, Step    10] loss: 0.343\n",
      "[Epoch 69, Step    20] loss: 0.321\n",
      "[Epoch 69, Step    30] loss: 0.320\n",
      "[Epoch 69, Step    40] loss: 0.342\n",
      "[Epoch 69, Step    50] loss: 0.344\n",
      "[Epoch 69, Step    60] loss: 0.356\n",
      "[Epoch 69, Step    70] loss: 0.330\n",
      "[Epoch 69, Step    80] loss: 0.345\n",
      "[Epoch 69, Step    90] loss: 0.317\n",
      "[Epoch 69, Step   100] loss: 0.335\n",
      "[Epoch 69, Step   110] loss: 0.342\n",
      "[Epoch 69, Step   120] loss: 0.345\n",
      "[Epoch 69, Step   130] loss: 0.316\n",
      "[Epoch 70, Step    10] loss: 0.332\n",
      "[Epoch 70, Step    20] loss: 0.319\n",
      "[Epoch 70, Step    30] loss: 0.330\n",
      "[Epoch 70, Step    40] loss: 0.355\n",
      "[Epoch 70, Step    50] loss: 0.354\n",
      "[Epoch 70, Step    60] loss: 0.381\n",
      "[Epoch 70, Step    70] loss: 0.321\n",
      "[Epoch 70, Step    80] loss: 0.347\n",
      "[Epoch 70, Step    90] loss: 0.319\n",
      "[Epoch 70, Step   100] loss: 0.320\n",
      "[Epoch 70, Step   110] loss: 0.319\n",
      "[Epoch 70, Step   120] loss: 0.316\n",
      "[Epoch 70, Step   130] loss: 0.343\n",
      "[Epoch 71, Step    10] loss: 0.342\n",
      "[Epoch 71, Step    20] loss: 0.330\n",
      "[Epoch 71, Step    30] loss: 0.330\n",
      "[Epoch 71, Step    40] loss: 0.319\n",
      "[Epoch 71, Step    50] loss: 0.335\n",
      "[Epoch 71, Step    60] loss: 0.355\n",
      "[Epoch 71, Step    70] loss: 0.332\n",
      "[Epoch 71, Step    80] loss: 0.341\n",
      "[Epoch 71, Step    90] loss: 0.321\n",
      "[Epoch 71, Step   100] loss: 0.356\n",
      "[Epoch 71, Step   110] loss: 0.343\n",
      "[Epoch 71, Step   120] loss: 0.320\n",
      "[Epoch 71, Step   130] loss: 0.330\n",
      "[Epoch 72, Step    10] loss: 0.331\n",
      "[Epoch 72, Step    20] loss: 0.332\n",
      "[Epoch 72, Step    30] loss: 0.331\n",
      "[Epoch 72, Step    40] loss: 0.318\n",
      "[Epoch 72, Step    50] loss: 0.343\n",
      "[Epoch 72, Step    60] loss: 0.354\n",
      "[Epoch 72, Step    70] loss: 0.323\n",
      "[Epoch 72, Step    80] loss: 0.331\n",
      "[Epoch 72, Step    90] loss: 0.337\n",
      "[Epoch 72, Step   100] loss: 0.343\n",
      "[Epoch 72, Step   110] loss: 0.342\n",
      "[Epoch 72, Step   120] loss: 0.343\n",
      "[Epoch 72, Step   130] loss: 0.329\n",
      "[Epoch 73, Step    10] loss: 0.330\n",
      "[Epoch 73, Step    20] loss: 0.319\n",
      "[Epoch 73, Step    30] loss: 0.332\n",
      "[Epoch 73, Step    40] loss: 0.332\n",
      "[Epoch 73, Step    50] loss: 0.318\n",
      "[Epoch 73, Step    60] loss: 0.331\n",
      "[Epoch 73, Step    70] loss: 0.349\n",
      "[Epoch 73, Step    80] loss: 0.357\n",
      "[Epoch 73, Step    90] loss: 0.319\n",
      "[Epoch 73, Step   100] loss: 0.355\n",
      "[Epoch 73, Step   110] loss: 0.343\n",
      "[Epoch 73, Step   120] loss: 0.354\n",
      "[Epoch 73, Step   130] loss: 0.317\n",
      "[Epoch 74, Step    10] loss: 0.333\n",
      "[Epoch 74, Step    20] loss: 0.319\n",
      "[Epoch 74, Step    30] loss: 0.332\n",
      "[Epoch 74, Step    40] loss: 0.331\n",
      "[Epoch 74, Step    50] loss: 0.343\n",
      "[Epoch 74, Step    60] loss: 0.343\n",
      "[Epoch 74, Step    70] loss: 0.343\n",
      "[Epoch 74, Step    80] loss: 0.330\n",
      "[Epoch 74, Step    90] loss: 0.357\n",
      "[Epoch 74, Step   100] loss: 0.335\n",
      "[Epoch 74, Step   110] loss: 0.354\n",
      "[Epoch 74, Step   120] loss: 0.318\n",
      "[Epoch 74, Step   130] loss: 0.318\n",
      "[Epoch 75, Step    10] loss: 0.343\n",
      "[Epoch 75, Step    20] loss: 0.318\n",
      "[Epoch 75, Step    30] loss: 0.354\n",
      "[Epoch 75, Step    40] loss: 0.330\n",
      "[Epoch 75, Step    50] loss: 0.342\n",
      "[Epoch 75, Step    60] loss: 0.332\n",
      "[Epoch 75, Step    70] loss: 0.331\n",
      "[Epoch 75, Step    80] loss: 0.336\n",
      "[Epoch 75, Step    90] loss: 0.331\n",
      "[Epoch 75, Step   100] loss: 0.357\n",
      "[Epoch 75, Step   110] loss: 0.333\n",
      "[Epoch 75, Step   120] loss: 0.332\n",
      "[Epoch 75, Step   130] loss: 0.318\n",
      "[Epoch 76, Step    10] loss: 0.345\n",
      "[Epoch 76, Step    20] loss: 0.330\n",
      "[Epoch 76, Step    30] loss: 0.320\n",
      "[Epoch 76, Step    40] loss: 0.368\n",
      "[Epoch 76, Step    50] loss: 0.333\n",
      "[Epoch 76, Step    60] loss: 0.344\n",
      "[Epoch 76, Step    70] loss: 0.344\n",
      "[Epoch 76, Step    80] loss: 0.343\n",
      "[Epoch 76, Step    90] loss: 0.318\n",
      "[Epoch 76, Step   100] loss: 0.342\n",
      "[Epoch 76, Step   110] loss: 0.335\n",
      "[Epoch 76, Step   120] loss: 0.319\n",
      "[Epoch 76, Step   130] loss: 0.317\n",
      "[Epoch 77, Step    10] loss: 0.344\n",
      "[Epoch 77, Step    20] loss: 0.344\n",
      "[Epoch 77, Step    30] loss: 0.319\n",
      "[Epoch 77, Step    40] loss: 0.330\n",
      "[Epoch 77, Step    50] loss: 0.318\n",
      "[Epoch 77, Step    60] loss: 0.346\n",
      "[Epoch 77, Step    70] loss: 0.330\n",
      "[Epoch 77, Step    80] loss: 0.332\n",
      "[Epoch 77, Step    90] loss: 0.330\n",
      "[Epoch 77, Step   100] loss: 0.331\n",
      "[Epoch 77, Step   110] loss: 0.344\n",
      "[Epoch 77, Step   120] loss: 0.347\n",
      "[Epoch 77, Step   130] loss: 0.341\n",
      "[Epoch 78, Step    10] loss: 0.319\n",
      "[Epoch 78, Step    20] loss: 0.347\n",
      "[Epoch 78, Step    30] loss: 0.356\n",
      "[Epoch 78, Step    40] loss: 0.357\n",
      "[Epoch 78, Step    50] loss: 0.318\n",
      "[Epoch 78, Step    60] loss: 0.330\n",
      "[Epoch 78, Step    70] loss: 0.331\n",
      "[Epoch 78, Step    80] loss: 0.318\n",
      "[Epoch 78, Step    90] loss: 0.344\n",
      "[Epoch 78, Step   100] loss: 0.330\n",
      "[Epoch 78, Step   110] loss: 0.330\n",
      "[Epoch 78, Step   120] loss: 0.355\n",
      "[Epoch 78, Step   130] loss: 0.318\n",
      "[Epoch 79, Step    10] loss: 0.344\n",
      "[Epoch 79, Step    20] loss: 0.331\n",
      "[Epoch 79, Step    30] loss: 0.355\n",
      "[Epoch 79, Step    40] loss: 0.332\n",
      "[Epoch 79, Step    50] loss: 0.318\n",
      "[Epoch 79, Step    60] loss: 0.344\n",
      "[Epoch 79, Step    70] loss: 0.316\n",
      "[Epoch 79, Step    80] loss: 0.322\n",
      "[Epoch 79, Step    90] loss: 0.358\n",
      "[Epoch 79, Step   100] loss: 0.345\n",
      "[Epoch 79, Step   110] loss: 0.332\n",
      "[Epoch 79, Step   120] loss: 0.317\n",
      "[Epoch 79, Step   130] loss: 0.342\n",
      "[Epoch 80, Step    10] loss: 0.318\n",
      "[Epoch 80, Step    20] loss: 0.330\n",
      "[Epoch 80, Step    30] loss: 0.318\n",
      "[Epoch 80, Step    40] loss: 0.380\n",
      "[Epoch 80, Step    50] loss: 0.319\n",
      "[Epoch 80, Step    60] loss: 0.343\n",
      "[Epoch 80, Step    70] loss: 0.319\n",
      "[Epoch 80, Step    80] loss: 0.343\n",
      "[Epoch 80, Step    90] loss: 0.334\n",
      "[Epoch 80, Step   100] loss: 0.343\n",
      "[Epoch 80, Step   110] loss: 0.370\n",
      "[Epoch 80, Step   120] loss: 0.319\n",
      "[Epoch 80, Step   130] loss: 0.321\n",
      "[Epoch 81, Step    10] loss: 0.355\n",
      "[Epoch 81, Step    20] loss: 0.335\n",
      "[Epoch 81, Step    30] loss: 0.344\n",
      "[Epoch 81, Step    40] loss: 0.317\n",
      "[Epoch 81, Step    50] loss: 0.342\n",
      "[Epoch 81, Step    60] loss: 0.318\n",
      "[Epoch 81, Step    70] loss: 0.355\n",
      "[Epoch 81, Step    80] loss: 0.331\n",
      "[Epoch 81, Step    90] loss: 0.331\n",
      "[Epoch 81, Step   100] loss: 0.344\n",
      "[Epoch 81, Step   110] loss: 0.333\n",
      "[Epoch 81, Step   120] loss: 0.318\n",
      "[Epoch 81, Step   130] loss: 0.331\n",
      "[Epoch 82, Step    10] loss: 0.332\n",
      "[Epoch 82, Step    20] loss: 0.344\n",
      "[Epoch 82, Step    30] loss: 0.332\n",
      "[Epoch 82, Step    40] loss: 0.318\n",
      "[Epoch 82, Step    50] loss: 0.368\n",
      "[Epoch 82, Step    60] loss: 0.317\n",
      "[Epoch 82, Step    70] loss: 0.367\n",
      "[Epoch 82, Step    80] loss: 0.332\n",
      "[Epoch 82, Step    90] loss: 0.317\n",
      "[Epoch 82, Step   100] loss: 0.342\n",
      "[Epoch 82, Step   110] loss: 0.334\n",
      "[Epoch 82, Step   120] loss: 0.320\n",
      "[Epoch 82, Step   130] loss: 0.334\n",
      "[Epoch 83, Step    10] loss: 0.319\n",
      "[Epoch 83, Step    20] loss: 0.332\n",
      "[Epoch 83, Step    30] loss: 0.328\n",
      "[Epoch 83, Step    40] loss: 0.333\n",
      "[Epoch 83, Step    50] loss: 0.354\n",
      "[Epoch 83, Step    60] loss: 0.345\n",
      "[Epoch 83, Step    70] loss: 0.323\n",
      "[Epoch 83, Step    80] loss: 0.329\n",
      "[Epoch 83, Step    90] loss: 0.343\n",
      "[Epoch 83, Step   100] loss: 0.357\n",
      "[Epoch 83, Step   110] loss: 0.319\n",
      "[Epoch 83, Step   120] loss: 0.330\n",
      "[Epoch 83, Step   130] loss: 0.343\n",
      "[Epoch 84, Step    10] loss: 0.343\n",
      "[Epoch 84, Step    20] loss: 0.342\n",
      "[Epoch 84, Step    30] loss: 0.331\n",
      "[Epoch 84, Step    40] loss: 0.319\n",
      "[Epoch 84, Step    50] loss: 0.331\n",
      "[Epoch 84, Step    60] loss: 0.356\n",
      "[Epoch 84, Step    70] loss: 0.330\n",
      "[Epoch 84, Step    80] loss: 0.317\n",
      "[Epoch 84, Step    90] loss: 0.357\n",
      "[Epoch 84, Step   100] loss: 0.335\n",
      "[Epoch 84, Step   110] loss: 0.331\n",
      "[Epoch 84, Step   120] loss: 0.330\n",
      "[Epoch 84, Step   130] loss: 0.339\n",
      "[Epoch 85, Step    10] loss: 0.355\n",
      "[Epoch 85, Step    20] loss: 0.318\n",
      "[Epoch 85, Step    30] loss: 0.318\n",
      "[Epoch 85, Step    40] loss: 0.331\n",
      "[Epoch 85, Step    50] loss: 0.330\n",
      "[Epoch 85, Step    60] loss: 0.317\n",
      "[Epoch 85, Step    70] loss: 0.356\n",
      "[Epoch 85, Step    80] loss: 0.343\n",
      "[Epoch 85, Step    90] loss: 0.344\n",
      "[Epoch 85, Step   100] loss: 0.331\n",
      "[Epoch 85, Step   110] loss: 0.326\n",
      "[Epoch 85, Step   120] loss: 0.354\n",
      "[Epoch 85, Step   130] loss: 0.332\n",
      "[Epoch 86, Step    10] loss: 0.332\n",
      "[Epoch 86, Step    20] loss: 0.323\n",
      "[Epoch 86, Step    30] loss: 0.354\n",
      "[Epoch 86, Step    40] loss: 0.333\n",
      "[Epoch 86, Step    50] loss: 0.342\n",
      "[Epoch 86, Step    60] loss: 0.342\n",
      "[Epoch 86, Step    70] loss: 0.343\n",
      "[Epoch 86, Step    80] loss: 0.331\n",
      "[Epoch 86, Step    90] loss: 0.344\n",
      "[Epoch 86, Step   100] loss: 0.332\n",
      "[Epoch 86, Step   110] loss: 0.329\n",
      "[Epoch 86, Step   120] loss: 0.318\n",
      "[Epoch 86, Step   130] loss: 0.331\n",
      "[Epoch 87, Step    10] loss: 0.336\n",
      "[Epoch 87, Step    20] loss: 0.317\n",
      "[Epoch 87, Step    30] loss: 0.359\n",
      "[Epoch 87, Step    40] loss: 0.355\n",
      "[Epoch 87, Step    50] loss: 0.317\n",
      "[Epoch 87, Step    60] loss: 0.319\n",
      "[Epoch 87, Step    70] loss: 0.341\n",
      "[Epoch 87, Step    80] loss: 0.320\n",
      "[Epoch 87, Step    90] loss: 0.330\n",
      "[Epoch 87, Step   100] loss: 0.330\n",
      "[Epoch 87, Step   110] loss: 0.332\n",
      "[Epoch 87, Step   120] loss: 0.355\n",
      "[Epoch 87, Step   130] loss: 0.344\n",
      "[Epoch 88, Step    10] loss: 0.344\n",
      "[Epoch 88, Step    20] loss: 0.346\n",
      "[Epoch 88, Step    30] loss: 0.331\n",
      "[Epoch 88, Step    40] loss: 0.332\n",
      "[Epoch 88, Step    50] loss: 0.341\n",
      "[Epoch 88, Step    60] loss: 0.332\n",
      "[Epoch 88, Step    70] loss: 0.331\n",
      "[Epoch 88, Step    80] loss: 0.319\n",
      "[Epoch 88, Step    90] loss: 0.346\n",
      "[Epoch 88, Step   100] loss: 0.330\n",
      "[Epoch 88, Step   110] loss: 0.318\n",
      "[Epoch 88, Step   120] loss: 0.342\n",
      "[Epoch 88, Step   130] loss: 0.342\n",
      "[Epoch 89, Step    10] loss: 0.344\n",
      "[Epoch 89, Step    20] loss: 0.355\n",
      "[Epoch 89, Step    30] loss: 0.344\n",
      "[Epoch 89, Step    40] loss: 0.332\n",
      "[Epoch 89, Step    50] loss: 0.342\n",
      "[Epoch 89, Step    60] loss: 0.335\n",
      "[Epoch 89, Step    70] loss: 0.319\n",
      "[Epoch 89, Step    80] loss: 0.329\n",
      "[Epoch 89, Step    90] loss: 0.344\n",
      "[Epoch 89, Step   100] loss: 0.317\n",
      "[Epoch 89, Step   110] loss: 0.319\n",
      "[Epoch 89, Step   120] loss: 0.320\n",
      "[Epoch 89, Step   130] loss: 0.355\n",
      "[Epoch 90, Step    10] loss: 0.345\n",
      "[Epoch 90, Step    20] loss: 0.342\n",
      "[Epoch 90, Step    30] loss: 0.332\n",
      "[Epoch 90, Step    40] loss: 0.356\n",
      "[Epoch 90, Step    50] loss: 0.324\n",
      "[Epoch 90, Step    60] loss: 0.316\n",
      "[Epoch 90, Step    70] loss: 0.334\n",
      "[Epoch 90, Step    80] loss: 0.341\n",
      "[Epoch 90, Step    90] loss: 0.331\n",
      "[Epoch 90, Step   100] loss: 0.317\n",
      "[Epoch 90, Step   110] loss: 0.343\n",
      "[Epoch 90, Step   120] loss: 0.343\n",
      "[Epoch 90, Step   130] loss: 0.330\n",
      "[Epoch 91, Step    10] loss: 0.319\n",
      "[Epoch 91, Step    20] loss: 0.329\n",
      "[Epoch 91, Step    30] loss: 0.356\n",
      "[Epoch 91, Step    40] loss: 0.318\n",
      "[Epoch 91, Step    50] loss: 0.318\n",
      "[Epoch 91, Step    60] loss: 0.317\n",
      "[Epoch 91, Step    70] loss: 0.348\n",
      "[Epoch 91, Step    80] loss: 0.346\n",
      "[Epoch 91, Step    90] loss: 0.330\n",
      "[Epoch 91, Step   100] loss: 0.332\n",
      "[Epoch 91, Step   110] loss: 0.330\n",
      "[Epoch 91, Step   120] loss: 0.381\n",
      "[Epoch 91, Step   130] loss: 0.332\n",
      "[Epoch 92, Step    10] loss: 0.343\n",
      "[Epoch 92, Step    20] loss: 0.330\n",
      "[Epoch 92, Step    30] loss: 0.344\n",
      "[Epoch 92, Step    40] loss: 0.329\n",
      "[Epoch 92, Step    50] loss: 0.341\n",
      "[Epoch 92, Step    60] loss: 0.331\n",
      "[Epoch 92, Step    70] loss: 0.332\n",
      "[Epoch 92, Step    80] loss: 0.318\n",
      "[Epoch 92, Step    90] loss: 0.344\n",
      "[Epoch 92, Step   100] loss: 0.336\n",
      "[Epoch 92, Step   110] loss: 0.344\n",
      "[Epoch 92, Step   120] loss: 0.345\n",
      "[Epoch 92, Step   130] loss: 0.317\n",
      "[Epoch 93, Step    10] loss: 0.380\n",
      "[Epoch 93, Step    20] loss: 0.341\n",
      "[Epoch 93, Step    30] loss: 0.343\n",
      "[Epoch 93, Step    40] loss: 0.334\n",
      "[Epoch 93, Step    50] loss: 0.332\n",
      "[Epoch 93, Step    60] loss: 0.335\n",
      "[Epoch 93, Step    70] loss: 0.317\n",
      "[Epoch 93, Step    80] loss: 0.333\n",
      "[Epoch 93, Step    90] loss: 0.331\n",
      "[Epoch 93, Step   100] loss: 0.317\n",
      "[Epoch 93, Step   110] loss: 0.332\n",
      "[Epoch 93, Step   120] loss: 0.318\n",
      "[Epoch 93, Step   130] loss: 0.343\n",
      "[Epoch 94, Step    10] loss: 0.368\n",
      "[Epoch 94, Step    20] loss: 0.323\n",
      "[Epoch 94, Step    30] loss: 0.356\n",
      "[Epoch 94, Step    40] loss: 0.342\n",
      "[Epoch 94, Step    50] loss: 0.321\n",
      "[Epoch 94, Step    60] loss: 0.366\n",
      "[Epoch 94, Step    70] loss: 0.319\n",
      "[Epoch 94, Step    80] loss: 0.318\n",
      "[Epoch 94, Step    90] loss: 0.330\n",
      "[Epoch 94, Step   100] loss: 0.320\n",
      "[Epoch 94, Step   110] loss: 0.331\n",
      "[Epoch 94, Step   120] loss: 0.318\n",
      "[Epoch 94, Step   130] loss: 0.343\n",
      "[Epoch 95, Step    10] loss: 0.342\n",
      "[Epoch 95, Step    20] loss: 0.360\n",
      "[Epoch 95, Step    30] loss: 0.355\n",
      "[Epoch 95, Step    40] loss: 0.332\n",
      "[Epoch 95, Step    50] loss: 0.320\n",
      "[Epoch 95, Step    60] loss: 0.330\n",
      "[Epoch 95, Step    70] loss: 0.319\n",
      "[Epoch 95, Step    80] loss: 0.329\n",
      "[Epoch 95, Step    90] loss: 0.318\n",
      "[Epoch 95, Step   100] loss: 0.334\n",
      "[Epoch 95, Step   110] loss: 0.331\n",
      "[Epoch 95, Step   120] loss: 0.354\n",
      "[Epoch 95, Step   130] loss: 0.332\n",
      "[Epoch 96, Step    10] loss: 0.361\n",
      "[Epoch 96, Step    20] loss: 0.323\n",
      "[Epoch 96, Step    30] loss: 0.333\n",
      "[Epoch 96, Step    40] loss: 0.318\n",
      "[Epoch 96, Step    50] loss: 0.330\n",
      "[Epoch 96, Step    60] loss: 0.331\n",
      "[Epoch 96, Step    70] loss: 0.341\n",
      "[Epoch 96, Step    80] loss: 0.318\n",
      "[Epoch 96, Step    90] loss: 0.346\n",
      "[Epoch 96, Step   100] loss: 0.355\n",
      "[Epoch 96, Step   110] loss: 0.342\n",
      "[Epoch 96, Step   120] loss: 0.329\n",
      "[Epoch 96, Step   130] loss: 0.330\n",
      "[Epoch 97, Step    10] loss: 0.330\n",
      "[Epoch 97, Step    20] loss: 0.319\n",
      "[Epoch 97, Step    30] loss: 0.330\n",
      "[Epoch 97, Step    40] loss: 0.344\n",
      "[Epoch 97, Step    50] loss: 0.319\n",
      "[Epoch 97, Step    60] loss: 0.330\n",
      "[Epoch 97, Step    70] loss: 0.333\n",
      "[Epoch 97, Step    80] loss: 0.381\n",
      "[Epoch 97, Step    90] loss: 0.342\n",
      "[Epoch 97, Step   100] loss: 0.319\n",
      "[Epoch 97, Step   110] loss: 0.334\n",
      "[Epoch 97, Step   120] loss: 0.357\n",
      "[Epoch 97, Step   130] loss: 0.318\n",
      "[Epoch 98, Step    10] loss: 0.342\n",
      "[Epoch 98, Step    20] loss: 0.333\n",
      "[Epoch 98, Step    30] loss: 0.330\n",
      "[Epoch 98, Step    40] loss: 0.344\n",
      "[Epoch 98, Step    50] loss: 0.344\n",
      "[Epoch 98, Step    60] loss: 0.329\n",
      "[Epoch 98, Step    70] loss: 0.318\n",
      "[Epoch 98, Step    80] loss: 0.342\n",
      "[Epoch 98, Step    90] loss: 0.331\n",
      "[Epoch 98, Step   100] loss: 0.330\n",
      "[Epoch 98, Step   110] loss: 0.331\n",
      "[Epoch 98, Step   120] loss: 0.349\n",
      "[Epoch 98, Step   130] loss: 0.332\n",
      "[Epoch 99, Step    10] loss: 0.354\n",
      "[Epoch 99, Step    20] loss: 0.329\n",
      "[Epoch 99, Step    30] loss: 0.318\n",
      "[Epoch 99, Step    40] loss: 0.330\n",
      "[Epoch 99, Step    50] loss: 0.319\n",
      "[Epoch 99, Step    60] loss: 0.357\n",
      "[Epoch 99, Step    70] loss: 0.344\n",
      "[Epoch 99, Step    80] loss: 0.344\n",
      "[Epoch 99, Step    90] loss: 0.319\n",
      "[Epoch 99, Step   100] loss: 0.330\n",
      "[Epoch 99, Step   110] loss: 0.362\n",
      "[Epoch 99, Step   120] loss: 0.318\n",
      "[Epoch 99, Step   130] loss: 0.330\n",
      "[Epoch 100, Step    10] loss: 0.344\n",
      "[Epoch 100, Step    20] loss: 0.343\n",
      "[Epoch 100, Step    30] loss: 0.343\n",
      "[Epoch 100, Step    40] loss: 0.329\n",
      "[Epoch 100, Step    50] loss: 0.332\n",
      "[Epoch 100, Step    60] loss: 0.331\n",
      "[Epoch 100, Step    70] loss: 0.343\n",
      "[Epoch 100, Step    80] loss: 0.321\n",
      "[Epoch 100, Step    90] loss: 0.360\n",
      "[Epoch 100, Step   100] loss: 0.342\n",
      "[Epoch 100, Step   110] loss: 0.319\n",
      "[Epoch 100, Step   120] loss: 0.318\n",
      "[Epoch 100, Step   130] loss: 0.330\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# define the initial learning rate here\n",
    "learning_rate = 1e-2\n",
    "n_epochs = 100 # how many epochs to run\n",
    "\n",
    "# define loss function\n",
    "#TODO: define loss function for this binary classification \n",
    "\n",
    "# Using Binary Cross-Entropy with Logits loss since it handles the sigmoid activation.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# initialize network parameters\n",
    "# print(f\"this is params: {params}\")\n",
    "# for layer in params:\n",
    "#     print(f\"Layer {layer} Weights requires_grad: {params[layer]['w'].requires_grad}\")\n",
    "#     print(f\"Layer {layer} Biases requires_grad: {params[layer]['b'].requires_grad}\")\n",
    "init_params(params)\n",
    "    \n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        labels = labels.float()\n",
    "        inputs = inputs.float()\n",
    "\n",
    "        # Forward \n",
    "        output = net(inputs, params, activations)\n",
    "        \n",
    "        # Compute the loss using the final output\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        params = backprop(loss, params, learning_rate)\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:  # print every 10 mini-batches\n",
    "            print('[Epoch %d, Step %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    # adjust learning rate\n",
    "    learning_rate = adjust_lr(learning_rate, epoch)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afab0be-e539-424a-a4f9-920c3e060451",
   "metadata": {},
   "source": [
    "**Evaluation**: Now testing with your trained model on the all test datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d95a316e-276e-4384-8934-74cc765a945f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 84 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# since you're not training, you don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        _, labels = torch.max(labels, 1)\n",
    "        \n",
    "        # calculate outputs by running images through the network\n",
    "        output = net(images, params, activations)\n",
    "\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac224c58-b815-4abc-baa6-25d08b07eb6b",
   "metadata": {},
   "source": [
    "**Evaluation**: Testing with your trained model on the each labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "405c09fb-93d6-4828-8f72-9dacc18adeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class panda is: 82.0 %\n",
      "Accuracy for class grizzly is: 86.8 %\n"
     ]
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        _, labels = torch.max(labels, 1)\n",
    "        output = net(images, params, activations)\n",
    "        _, predictions = torch.max(output, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                         accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d54c5a-aec2-4512-99a4-3f76d4c543b8",
   "metadata": {},
   "source": [
    "# Part II\n",
    "\n",
    "---\n",
    "\n",
    "## Convolutional neural network.\n",
    "\n",
    "\n",
    "**Architecture**:\n",
    "\n",
    "1. CNN Layer1\n",
    "2. CNN Layer2\n",
    "3. Pooling Layer\n",
    "4. FC layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa3df118-d086-45e7-8684-8e901a0c5d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIGNORE: NOTES FOR PERSONAL USE\\n\\nthe output volume as a function of the input volume size (W), the receptive field size of the Conv Layer neurons (F), \\nthe stride with which they are applied (S), and the amount of zero padding used (P) on the border. \\n\\nthe Conv Layer:\\n\\nAccepts a volume of size W1×H1×D1\\nRequires four hyperparameters:\\nNumber of filters K, their spatial extent F,the stride S ,the amount of zero padding P.\\nProduces a volume of size W2×H2×D2\\n where:\\nW2=(W1−F+2P)/S+1\\nH2=(H1−F+2P)/S+1\\n (i.e. width and height are computed equally by symmetry)\\n\\npooling layer: Accepts a volume of size W1×H1×D1\\nRequires two hyperparameters:\\ntheir spatial extent F,the stride S ,Produces a volume of size W2×H2×D2\\nwhere:\\nW2=(W1−F)/S+1\\nH2=(H1−F)/S+1\\nD2=D1\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "IGNORE: NOTES FOR PERSONAL USE\n",
    "\n",
    "the output volume as a function of the input volume size (W), the receptive field size of the Conv Layer neurons (F), \n",
    "the stride with which they are applied (S), and the amount of zero padding used (P) on the border. \n",
    "\n",
    "the Conv Layer:\n",
    "\n",
    "Accepts a volume of size W1×H1×D1\n",
    "Requires four hyperparameters:\n",
    "Number of filters K, their spatial extent F,the stride S ,the amount of zero padding P.\n",
    "Produces a volume of size W2×H2×D2\n",
    " where:\n",
    "W2=(W1−F+2P)/S+1\n",
    "H2=(H1−F+2P)/S+1\n",
    " (i.e. width and height are computed equally by symmetry)\n",
    "\n",
    "pooling layer: Accepts a volume of size W1×H1×D1\n",
    "Requires two hyperparameters:\n",
    "their spatial extent F,the stride S ,Produces a volume of size W2×H2×D2\n",
    "where:\n",
    "W2=(W1−F)/S+1\n",
    "H2=(H1−F)/S+1\n",
    "D2=D1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2cf3e263-6eba-4d52-8aea-ec13531bfaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # the conv layer is a layer of small filters we apply to the image, does most of the heavy lifting and produces a 2-d activation map\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=5, padding=2) # compute the weights between neurons and thier regions\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # downsize the width and height\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(in_features=256*14*14, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=2) # compute class scores for the two classses \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) # uses relu activation method \n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 256*14*14)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        output = torch.sigmoid(self.fc2(x))  # Using sigmoid activation for the final layer\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af25bde7-74c3-4654-85af-cf58008f8e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Step    10] loss: 0.686\n",
      "[Epoch 1, Step    20] loss: 0.686\n",
      "[Epoch 1, Step    30] loss: 0.680\n",
      "[Epoch 1, Step    40] loss: 0.674\n",
      "[Epoch 1, Step    50] loss: 0.645\n",
      "[Epoch 1, Step    60] loss: 0.631\n",
      "[Epoch 1, Step    70] loss: 0.648\n",
      "[Epoch 1, Step    80] loss: 0.616\n",
      "[Epoch 1, Step    90] loss: 0.573\n",
      "[Epoch 1, Step   100] loss: 0.637\n",
      "[Epoch 1, Step   110] loss: 0.525\n",
      "[Epoch 1, Step   120] loss: 0.615\n",
      "[Epoch 1, Step   130] loss: 0.588\n",
      "[Epoch 2, Step    10] loss: 0.520\n",
      "[Epoch 2, Step    20] loss: 0.525\n",
      "[Epoch 2, Step    30] loss: 0.587\n",
      "[Epoch 2, Step    40] loss: 0.532\n",
      "[Epoch 2, Step    50] loss: 0.563\n",
      "[Epoch 2, Step    60] loss: 0.486\n",
      "[Epoch 2, Step    70] loss: 0.464\n",
      "[Epoch 2, Step    80] loss: 0.525\n",
      "[Epoch 2, Step    90] loss: 0.513\n",
      "[Epoch 2, Step   100] loss: 0.414\n",
      "[Epoch 2, Step   110] loss: 0.544\n",
      "[Epoch 2, Step   120] loss: 0.485\n",
      "[Epoch 2, Step   130] loss: 0.512\n",
      "[Epoch 3, Step    10] loss: 0.345\n",
      "[Epoch 3, Step    20] loss: 0.389\n",
      "[Epoch 3, Step    30] loss: 0.346\n",
      "[Epoch 3, Step    40] loss: 0.474\n",
      "[Epoch 3, Step    50] loss: 0.500\n",
      "[Epoch 3, Step    60] loss: 0.457\n",
      "[Epoch 3, Step    70] loss: 0.435\n",
      "[Epoch 3, Step    80] loss: 0.528\n",
      "[Epoch 3, Step    90] loss: 0.538\n",
      "[Epoch 3, Step   100] loss: 0.393\n",
      "[Epoch 3, Step   110] loss: 0.404\n",
      "[Epoch 3, Step   120] loss: 0.519\n",
      "[Epoch 3, Step   130] loss: 0.403\n",
      "[Epoch 4, Step    10] loss: 0.394\n",
      "[Epoch 4, Step    20] loss: 0.348\n",
      "[Epoch 4, Step    30] loss: 0.321\n",
      "[Epoch 4, Step    40] loss: 0.454\n",
      "[Epoch 4, Step    50] loss: 0.330\n",
      "[Epoch 4, Step    60] loss: 0.519\n",
      "[Epoch 4, Step    70] loss: 0.387\n",
      "[Epoch 4, Step    80] loss: 0.473\n",
      "[Epoch 4, Step    90] loss: 0.293\n",
      "[Epoch 4, Step   100] loss: 0.360\n",
      "[Epoch 4, Step   110] loss: 0.395\n",
      "[Epoch 4, Step   120] loss: 0.313\n",
      "[Epoch 4, Step   130] loss: 0.364\n",
      "[Epoch 5, Step    10] loss: 0.296\n",
      "[Epoch 5, Step    20] loss: 0.319\n",
      "[Epoch 5, Step    30] loss: 0.237\n",
      "[Epoch 5, Step    40] loss: 0.480\n",
      "[Epoch 5, Step    50] loss: 0.402\n",
      "[Epoch 5, Step    60] loss: 0.443\n",
      "[Epoch 5, Step    70] loss: 0.297\n",
      "[Epoch 5, Step    80] loss: 0.340\n",
      "[Epoch 5, Step    90] loss: 0.425\n",
      "[Epoch 5, Step   100] loss: 0.377\n",
      "[Epoch 5, Step   110] loss: 0.285\n",
      "[Epoch 5, Step   120] loss: 0.405\n",
      "[Epoch 5, Step   130] loss: 0.394\n",
      "[Epoch 6, Step    10] loss: 0.262\n",
      "[Epoch 6, Step    20] loss: 0.263\n",
      "[Epoch 6, Step    30] loss: 0.353\n",
      "[Epoch 6, Step    40] loss: 0.293\n",
      "[Epoch 6, Step    50] loss: 0.316\n",
      "[Epoch 6, Step    60] loss: 0.366\n",
      "[Epoch 6, Step    70] loss: 0.306\n",
      "[Epoch 6, Step    80] loss: 0.352\n",
      "[Epoch 6, Step    90] loss: 0.437\n",
      "[Epoch 6, Step   100] loss: 0.263\n",
      "[Epoch 6, Step   110] loss: 0.308\n",
      "[Epoch 6, Step   120] loss: 0.360\n",
      "[Epoch 6, Step   130] loss: 0.275\n",
      "[Epoch 7, Step    10] loss: 0.315\n",
      "[Epoch 7, Step    20] loss: 0.255\n",
      "[Epoch 7, Step    30] loss: 0.260\n",
      "[Epoch 7, Step    40] loss: 0.360\n",
      "[Epoch 7, Step    50] loss: 0.269\n",
      "[Epoch 7, Step    60] loss: 0.373\n",
      "[Epoch 7, Step    70] loss: 0.223\n",
      "[Epoch 7, Step    80] loss: 0.338\n",
      "[Epoch 7, Step    90] loss: 0.257\n",
      "[Epoch 7, Step   100] loss: 0.280\n",
      "[Epoch 7, Step   110] loss: 0.270\n",
      "[Epoch 7, Step   120] loss: 0.266\n",
      "[Epoch 7, Step   130] loss: 0.244\n",
      "[Epoch 8, Step    10] loss: 0.244\n",
      "[Epoch 8, Step    20] loss: 0.243\n",
      "[Epoch 8, Step    30] loss: 0.330\n",
      "[Epoch 8, Step    40] loss: 0.226\n",
      "[Epoch 8, Step    50] loss: 0.243\n",
      "[Epoch 8, Step    60] loss: 0.287\n",
      "[Epoch 8, Step    70] loss: 0.296\n",
      "[Epoch 8, Step    80] loss: 0.185\n",
      "[Epoch 8, Step    90] loss: 0.294\n",
      "[Epoch 8, Step   100] loss: 0.188\n",
      "[Epoch 8, Step   110] loss: 0.283\n",
      "[Epoch 8, Step   120] loss: 0.206\n",
      "[Epoch 8, Step   130] loss: 0.313\n",
      "[Epoch 9, Step    10] loss: 0.209\n",
      "[Epoch 9, Step    20] loss: 0.260\n",
      "[Epoch 9, Step    30] loss: 0.229\n",
      "[Epoch 9, Step    40] loss: 0.210\n",
      "[Epoch 9, Step    50] loss: 0.243\n",
      "[Epoch 9, Step    60] loss: 0.279\n",
      "[Epoch 9, Step    70] loss: 0.205\n",
      "[Epoch 9, Step    80] loss: 0.215\n",
      "[Epoch 9, Step    90] loss: 0.256\n",
      "[Epoch 9, Step   100] loss: 0.252\n",
      "[Epoch 9, Step   110] loss: 0.247\n",
      "[Epoch 9, Step   120] loss: 0.222\n",
      "[Epoch 9, Step   130] loss: 0.202\n",
      "[Epoch 10, Step    10] loss: 0.194\n",
      "[Epoch 10, Step    20] loss: 0.202\n",
      "[Epoch 10, Step    30] loss: 0.145\n",
      "[Epoch 10, Step    40] loss: 0.210\n",
      "[Epoch 10, Step    50] loss: 0.169\n",
      "[Epoch 10, Step    60] loss: 0.150\n",
      "[Epoch 10, Step    70] loss: 0.280\n",
      "[Epoch 10, Step    80] loss: 0.185\n",
      "[Epoch 10, Step    90] loss: 0.196\n",
      "[Epoch 10, Step   100] loss: 0.207\n",
      "[Epoch 10, Step   110] loss: 0.299\n",
      "[Epoch 10, Step   120] loss: 0.201\n",
      "[Epoch 10, Step   130] loss: 0.239\n",
      "[Epoch 11, Step    10] loss: 0.178\n",
      "[Epoch 11, Step    20] loss: 0.201\n",
      "[Epoch 11, Step    30] loss: 0.179\n",
      "[Epoch 11, Step    40] loss: 0.223\n",
      "[Epoch 11, Step    50] loss: 0.162\n",
      "[Epoch 11, Step    60] loss: 0.154\n",
      "[Epoch 11, Step    70] loss: 0.235\n",
      "[Epoch 11, Step    80] loss: 0.210\n",
      "[Epoch 11, Step    90] loss: 0.270\n",
      "[Epoch 11, Step   100] loss: 0.193\n",
      "[Epoch 11, Step   110] loss: 0.203\n",
      "[Epoch 11, Step   120] loss: 0.172\n",
      "[Epoch 11, Step   130] loss: 0.200\n",
      "[Epoch 12, Step    10] loss: 0.183\n",
      "[Epoch 12, Step    20] loss: 0.199\n",
      "[Epoch 12, Step    30] loss: 0.250\n",
      "[Epoch 12, Step    40] loss: 0.168\n",
      "[Epoch 12, Step    50] loss: 0.141\n",
      "[Epoch 12, Step    60] loss: 0.180\n",
      "[Epoch 12, Step    70] loss: 0.106\n",
      "[Epoch 12, Step    80] loss: 0.255\n",
      "[Epoch 12, Step    90] loss: 0.176\n",
      "[Epoch 12, Step   100] loss: 0.218\n",
      "[Epoch 12, Step   110] loss: 0.196\n",
      "[Epoch 12, Step   120] loss: 0.162\n",
      "[Epoch 12, Step   130] loss: 0.106\n",
      "[Epoch 13, Step    10] loss: 0.192\n",
      "[Epoch 13, Step    20] loss: 0.263\n",
      "[Epoch 13, Step    30] loss: 0.185\n",
      "[Epoch 13, Step    40] loss: 0.187\n",
      "[Epoch 13, Step    50] loss: 0.168\n",
      "[Epoch 13, Step    60] loss: 0.161\n",
      "[Epoch 13, Step    70] loss: 0.170\n",
      "[Epoch 13, Step    80] loss: 0.203\n",
      "[Epoch 13, Step    90] loss: 0.148\n",
      "[Epoch 13, Step   100] loss: 0.125\n",
      "[Epoch 13, Step   110] loss: 0.175\n",
      "[Epoch 13, Step   120] loss: 0.100\n",
      "[Epoch 13, Step   130] loss: 0.123\n",
      "[Epoch 14, Step    10] loss: 0.237\n",
      "[Epoch 14, Step    20] loss: 0.158\n",
      "[Epoch 14, Step    30] loss: 0.133\n",
      "[Epoch 14, Step    40] loss: 0.250\n",
      "[Epoch 14, Step    50] loss: 0.157\n",
      "[Epoch 14, Step    60] loss: 0.135\n",
      "[Epoch 14, Step    70] loss: 0.114\n",
      "[Epoch 14, Step    80] loss: 0.137\n",
      "[Epoch 14, Step    90] loss: 0.147\n",
      "[Epoch 14, Step   100] loss: 0.098\n",
      "[Epoch 14, Step   110] loss: 0.136\n",
      "[Epoch 14, Step   120] loss: 0.112\n",
      "[Epoch 14, Step   130] loss: 0.200\n",
      "[Epoch 15, Step    10] loss: 0.212\n",
      "[Epoch 15, Step    20] loss: 0.188\n",
      "[Epoch 15, Step    30] loss: 0.094\n",
      "[Epoch 15, Step    40] loss: 0.158\n",
      "[Epoch 15, Step    50] loss: 0.112\n",
      "[Epoch 15, Step    60] loss: 0.134\n",
      "[Epoch 15, Step    70] loss: 0.132\n",
      "[Epoch 15, Step    80] loss: 0.160\n",
      "[Epoch 15, Step    90] loss: 0.099\n",
      "[Epoch 15, Step   100] loss: 0.103\n",
      "[Epoch 15, Step   110] loss: 0.159\n",
      "[Epoch 15, Step   120] loss: 0.150\n",
      "[Epoch 15, Step   130] loss: 0.105\n",
      "[Epoch 16, Step    10] loss: 0.098\n",
      "[Epoch 16, Step    20] loss: 0.144\n",
      "[Epoch 16, Step    30] loss: 0.130\n",
      "[Epoch 16, Step    40] loss: 0.078\n",
      "[Epoch 16, Step    50] loss: 0.089\n",
      "[Epoch 16, Step    60] loss: 0.130\n",
      "[Epoch 16, Step    70] loss: 0.086\n",
      "[Epoch 16, Step    80] loss: 0.093\n",
      "[Epoch 16, Step    90] loss: 0.162\n",
      "[Epoch 16, Step   100] loss: 0.170\n",
      "[Epoch 16, Step   110] loss: 0.165\n",
      "[Epoch 16, Step   120] loss: 0.129\n",
      "[Epoch 16, Step   130] loss: 0.134\n",
      "[Epoch 17, Step    10] loss: 0.139\n",
      "[Epoch 17, Step    20] loss: 0.096\n",
      "[Epoch 17, Step    30] loss: 0.067\n",
      "[Epoch 17, Step    40] loss: 0.135\n",
      "[Epoch 17, Step    50] loss: 0.076\n",
      "[Epoch 17, Step    60] loss: 0.168\n",
      "[Epoch 17, Step    70] loss: 0.092\n",
      "[Epoch 17, Step    80] loss: 0.111\n",
      "[Epoch 17, Step    90] loss: 0.122\n",
      "[Epoch 17, Step   100] loss: 0.099\n",
      "[Epoch 17, Step   110] loss: 0.100\n",
      "[Epoch 17, Step   120] loss: 0.125\n",
      "[Epoch 17, Step   130] loss: 0.085\n",
      "[Epoch 18, Step    10] loss: 0.073\n",
      "[Epoch 18, Step    20] loss: 0.049\n",
      "[Epoch 18, Step    30] loss: 0.070\n",
      "[Epoch 18, Step    40] loss: 0.099\n",
      "[Epoch 18, Step    50] loss: 0.145\n",
      "[Epoch 18, Step    60] loss: 0.103\n",
      "[Epoch 18, Step    70] loss: 0.130\n",
      "[Epoch 18, Step    80] loss: 0.033\n",
      "[Epoch 18, Step    90] loss: 0.082\n",
      "[Epoch 18, Step   100] loss: 0.125\n",
      "[Epoch 18, Step   110] loss: 0.089\n",
      "[Epoch 18, Step   120] loss: 0.083\n",
      "[Epoch 18, Step   130] loss: 0.161\n",
      "[Epoch 19, Step    10] loss: 0.080\n",
      "[Epoch 19, Step    20] loss: 0.057\n",
      "[Epoch 19, Step    30] loss: 0.172\n",
      "[Epoch 19, Step    40] loss: 0.080\n",
      "[Epoch 19, Step    50] loss: 0.124\n",
      "[Epoch 19, Step    60] loss: 0.151\n",
      "[Epoch 19, Step    70] loss: 0.117\n",
      "[Epoch 19, Step    80] loss: 0.042\n",
      "[Epoch 19, Step    90] loss: 0.119\n",
      "[Epoch 19, Step   100] loss: 0.079\n",
      "[Epoch 19, Step   110] loss: 0.081\n",
      "[Epoch 19, Step   120] loss: 0.047\n",
      "[Epoch 19, Step   130] loss: 0.042\n",
      "[Epoch 20, Step    10] loss: 0.181\n",
      "[Epoch 20, Step    20] loss: 0.079\n",
      "[Epoch 20, Step    30] loss: 0.099\n",
      "[Epoch 20, Step    40] loss: 0.112\n",
      "[Epoch 20, Step    50] loss: 0.082\n",
      "[Epoch 20, Step    60] loss: 0.079\n",
      "[Epoch 20, Step    70] loss: 0.053\n",
      "[Epoch 20, Step    80] loss: 0.033\n",
      "[Epoch 20, Step    90] loss: 0.072\n",
      "[Epoch 20, Step   100] loss: 0.023\n",
      "[Epoch 20, Step   110] loss: 0.073\n",
      "[Epoch 20, Step   120] loss: 0.093\n",
      "[Epoch 20, Step   130] loss: 0.196\n",
      "[Epoch 21, Step    10] loss: 0.142\n",
      "[Epoch 21, Step    20] loss: 0.045\n",
      "[Epoch 21, Step    30] loss: 0.044\n",
      "[Epoch 21, Step    40] loss: 0.066\n",
      "[Epoch 21, Step    50] loss: 0.058\n",
      "[Epoch 21, Step    60] loss: 0.105\n",
      "[Epoch 21, Step    70] loss: 0.126\n",
      "[Epoch 21, Step    80] loss: 0.055\n",
      "[Epoch 21, Step    90] loss: 0.046\n",
      "[Epoch 21, Step   100] loss: 0.077\n",
      "[Epoch 21, Step   110] loss: 0.076\n",
      "[Epoch 21, Step   120] loss: 0.054\n",
      "[Epoch 21, Step   130] loss: 0.043\n",
      "[Epoch 22, Step    10] loss: 0.037\n",
      "[Epoch 22, Step    20] loss: 0.029\n",
      "[Epoch 22, Step    30] loss: 0.025\n",
      "[Epoch 22, Step    40] loss: 0.087\n",
      "[Epoch 22, Step    50] loss: 0.076\n",
      "[Epoch 22, Step    60] loss: 0.082\n",
      "[Epoch 22, Step    70] loss: 0.056\n",
      "[Epoch 22, Step    80] loss: 0.084\n",
      "[Epoch 22, Step    90] loss: 0.045\n",
      "[Epoch 22, Step   100] loss: 0.065\n",
      "[Epoch 22, Step   110] loss: 0.043\n",
      "[Epoch 22, Step   120] loss: 0.131\n",
      "[Epoch 22, Step   130] loss: 0.053\n",
      "[Epoch 23, Step    10] loss: 0.035\n",
      "[Epoch 23, Step    20] loss: 0.099\n",
      "[Epoch 23, Step    30] loss: 0.055\n",
      "[Epoch 23, Step    40] loss: 0.059\n",
      "[Epoch 23, Step    50] loss: 0.030\n",
      "[Epoch 23, Step    60] loss: 0.037\n",
      "[Epoch 23, Step    70] loss: 0.051\n",
      "[Epoch 23, Step    80] loss: 0.041\n",
      "[Epoch 23, Step    90] loss: 0.076\n",
      "[Epoch 23, Step   100] loss: 0.133\n",
      "[Epoch 23, Step   110] loss: 0.047\n",
      "[Epoch 23, Step   120] loss: 0.041\n",
      "[Epoch 23, Step   130] loss: 0.040\n",
      "[Epoch 24, Step    10] loss: 0.071\n",
      "[Epoch 24, Step    20] loss: 0.073\n",
      "[Epoch 24, Step    30] loss: 0.034\n",
      "[Epoch 24, Step    40] loss: 0.048\n",
      "[Epoch 24, Step    50] loss: 0.039\n",
      "[Epoch 24, Step    60] loss: 0.058\n",
      "[Epoch 24, Step    70] loss: 0.054\n",
      "[Epoch 24, Step    80] loss: 0.031\n",
      "[Epoch 24, Step    90] loss: 0.014\n",
      "[Epoch 24, Step   100] loss: 0.027\n",
      "[Epoch 24, Step   110] loss: 0.035\n",
      "[Epoch 24, Step   120] loss: 0.059\n",
      "[Epoch 24, Step   130] loss: 0.137\n",
      "[Epoch 25, Step    10] loss: 0.070\n",
      "[Epoch 25, Step    20] loss: 0.048\n",
      "[Epoch 25, Step    30] loss: 0.057\n",
      "[Epoch 25, Step    40] loss: 0.042\n",
      "[Epoch 25, Step    50] loss: 0.017\n",
      "[Epoch 25, Step    60] loss: 0.024\n",
      "[Epoch 25, Step    70] loss: 0.032\n",
      "[Epoch 25, Step    80] loss: 0.046\n",
      "[Epoch 25, Step    90] loss: 0.051\n",
      "[Epoch 25, Step   100] loss: 0.091\n",
      "[Epoch 25, Step   110] loss: 0.033\n",
      "[Epoch 25, Step   120] loss: 0.046\n",
      "[Epoch 25, Step   130] loss: 0.008\n",
      "[Epoch 26, Step    10] loss: 0.023\n",
      "[Epoch 26, Step    20] loss: 0.015\n",
      "[Epoch 26, Step    30] loss: 0.041\n",
      "[Epoch 26, Step    40] loss: 0.075\n",
      "[Epoch 26, Step    50] loss: 0.048\n",
      "[Epoch 26, Step    60] loss: 0.021\n",
      "[Epoch 26, Step    70] loss: 0.042\n",
      "[Epoch 26, Step    80] loss: 0.055\n",
      "[Epoch 26, Step    90] loss: 0.027\n",
      "[Epoch 26, Step   100] loss: 0.034\n",
      "[Epoch 26, Step   110] loss: 0.041\n",
      "[Epoch 26, Step   120] loss: 0.050\n",
      "[Epoch 26, Step   130] loss: 0.036\n",
      "[Epoch 27, Step    10] loss: 0.043\n",
      "[Epoch 27, Step    20] loss: 0.054\n",
      "[Epoch 27, Step    30] loss: 0.015\n",
      "[Epoch 27, Step    40] loss: 0.017\n",
      "[Epoch 27, Step    50] loss: 0.026\n",
      "[Epoch 27, Step    60] loss: 0.020\n",
      "[Epoch 27, Step    70] loss: 0.025\n",
      "[Epoch 27, Step    80] loss: 0.040\n",
      "[Epoch 27, Step    90] loss: 0.013\n",
      "[Epoch 27, Step   100] loss: 0.023\n",
      "[Epoch 27, Step   110] loss: 0.030\n",
      "[Epoch 27, Step   120] loss: 0.081\n",
      "[Epoch 27, Step   130] loss: 0.026\n",
      "[Epoch 28, Step    10] loss: 0.025\n",
      "[Epoch 28, Step    20] loss: 0.011\n",
      "[Epoch 28, Step    30] loss: 0.035\n",
      "[Epoch 28, Step    40] loss: 0.021\n",
      "[Epoch 28, Step    50] loss: 0.028\n",
      "[Epoch 28, Step    60] loss: 0.037\n",
      "[Epoch 28, Step    70] loss: 0.051\n",
      "[Epoch 28, Step    80] loss: 0.029\n",
      "[Epoch 28, Step    90] loss: 0.024\n",
      "[Epoch 28, Step   100] loss: 0.026\n",
      "[Epoch 28, Step   110] loss: 0.026\n",
      "[Epoch 28, Step   120] loss: 0.043\n",
      "[Epoch 28, Step   130] loss: 0.047\n",
      "[Epoch 29, Step    10] loss: 0.039\n",
      "[Epoch 29, Step    20] loss: 0.032\n",
      "[Epoch 29, Step    30] loss: 0.055\n",
      "[Epoch 29, Step    40] loss: 0.012\n",
      "[Epoch 29, Step    50] loss: 0.014\n",
      "[Epoch 29, Step    60] loss: 0.017\n",
      "[Epoch 29, Step    70] loss: 0.011\n",
      "[Epoch 29, Step    80] loss: 0.035\n",
      "[Epoch 29, Step    90] loss: 0.015\n",
      "[Epoch 29, Step   100] loss: 0.020\n",
      "[Epoch 29, Step   110] loss: 0.015\n",
      "[Epoch 29, Step   120] loss: 0.032\n",
      "[Epoch 29, Step   130] loss: 0.064\n",
      "[Epoch 30, Step    10] loss: 0.180\n",
      "[Epoch 30, Step    20] loss: 0.035\n",
      "[Epoch 30, Step    30] loss: 0.016\n",
      "[Epoch 30, Step    40] loss: 0.030\n",
      "[Epoch 30, Step    50] loss: 0.025\n",
      "[Epoch 30, Step    60] loss: 0.016\n",
      "[Epoch 30, Step    70] loss: 0.010\n",
      "[Epoch 30, Step    80] loss: 0.037\n",
      "[Epoch 30, Step    90] loss: 0.024\n",
      "[Epoch 30, Step   100] loss: 0.011\n",
      "[Epoch 30, Step   110] loss: 0.020\n",
      "[Epoch 30, Step   120] loss: 0.035\n",
      "[Epoch 30, Step   130] loss: 0.014\n",
      "[Epoch 31, Step    10] loss: 0.023\n",
      "[Epoch 31, Step    20] loss: 0.023\n",
      "[Epoch 31, Step    30] loss: 0.018\n",
      "[Epoch 31, Step    40] loss: 0.020\n",
      "[Epoch 31, Step    50] loss: 0.029\n",
      "[Epoch 31, Step    60] loss: 0.034\n",
      "[Epoch 31, Step    70] loss: 0.015\n",
      "[Epoch 31, Step    80] loss: 0.017\n",
      "[Epoch 31, Step    90] loss: 0.013\n",
      "[Epoch 31, Step   100] loss: 0.009\n",
      "[Epoch 31, Step   110] loss: 0.018\n",
      "[Epoch 31, Step   120] loss: 0.010\n",
      "[Epoch 31, Step   130] loss: 0.011\n",
      "[Epoch 32, Step    10] loss: 0.017\n",
      "[Epoch 32, Step    20] loss: 0.052\n",
      "[Epoch 32, Step    30] loss: 0.009\n",
      "[Epoch 32, Step    40] loss: 0.023\n",
      "[Epoch 32, Step    50] loss: 0.008\n",
      "[Epoch 32, Step    60] loss: 0.034\n",
      "[Epoch 32, Step    70] loss: 0.019\n",
      "[Epoch 32, Step    80] loss: 0.008\n",
      "[Epoch 32, Step    90] loss: 0.012\n",
      "[Epoch 32, Step   100] loss: 0.008\n",
      "[Epoch 32, Step   110] loss: 0.012\n",
      "[Epoch 32, Step   120] loss: 0.008\n",
      "[Epoch 32, Step   130] loss: 0.016\n",
      "[Epoch 33, Step    10] loss: 0.013\n",
      "[Epoch 33, Step    20] loss: 0.029\n",
      "[Epoch 33, Step    30] loss: 0.015\n",
      "[Epoch 33, Step    40] loss: 0.008\n",
      "[Epoch 33, Step    50] loss: 0.005\n",
      "[Epoch 33, Step    60] loss: 0.016\n",
      "[Epoch 33, Step    70] loss: 0.017\n",
      "[Epoch 33, Step    80] loss: 0.015\n",
      "[Epoch 33, Step    90] loss: 0.012\n",
      "[Epoch 33, Step   100] loss: 0.045\n",
      "[Epoch 33, Step   110] loss: 0.015\n",
      "[Epoch 33, Step   120] loss: 0.011\n",
      "[Epoch 33, Step   130] loss: 0.027\n",
      "[Epoch 34, Step    10] loss: 0.016\n",
      "[Epoch 34, Step    20] loss: 0.007\n",
      "[Epoch 34, Step    30] loss: 0.003\n",
      "[Epoch 34, Step    40] loss: 0.007\n",
      "[Epoch 34, Step    50] loss: 0.011\n",
      "[Epoch 34, Step    60] loss: 0.025\n",
      "[Epoch 34, Step    70] loss: 0.017\n",
      "[Epoch 34, Step    80] loss: 0.010\n",
      "[Epoch 34, Step    90] loss: 0.013\n",
      "[Epoch 34, Step   100] loss: 0.017\n",
      "[Epoch 34, Step   110] loss: 0.026\n",
      "[Epoch 34, Step   120] loss: 0.026\n",
      "[Epoch 34, Step   130] loss: 0.010\n",
      "[Epoch 35, Step    10] loss: 0.010\n",
      "[Epoch 35, Step    20] loss: 0.005\n",
      "[Epoch 35, Step    30] loss: 0.010\n",
      "[Epoch 35, Step    40] loss: 0.008\n",
      "[Epoch 35, Step    50] loss: 0.014\n",
      "[Epoch 35, Step    60] loss: 0.023\n",
      "[Epoch 35, Step    70] loss: 0.029\n",
      "[Epoch 35, Step    80] loss: 0.010\n",
      "[Epoch 35, Step    90] loss: 0.006\n",
      "[Epoch 35, Step   100] loss: 0.018\n",
      "[Epoch 35, Step   110] loss: 0.009\n",
      "[Epoch 35, Step   120] loss: 0.007\n",
      "[Epoch 35, Step   130] loss: 0.009\n",
      "[Epoch 36, Step    10] loss: 0.009\n",
      "[Epoch 36, Step    20] loss: 0.010\n",
      "[Epoch 36, Step    30] loss: 0.010\n",
      "[Epoch 36, Step    40] loss: 0.004\n",
      "[Epoch 36, Step    50] loss: 0.018\n",
      "[Epoch 36, Step    60] loss: 0.016\n",
      "[Epoch 36, Step    70] loss: 0.009\n",
      "[Epoch 36, Step    80] loss: 0.013\n",
      "[Epoch 36, Step    90] loss: 0.004\n",
      "[Epoch 36, Step   100] loss: 0.007\n",
      "[Epoch 36, Step   110] loss: 0.007\n",
      "[Epoch 36, Step   120] loss: 0.004\n",
      "[Epoch 36, Step   130] loss: 0.010\n",
      "[Epoch 37, Step    10] loss: 0.009\n",
      "[Epoch 37, Step    20] loss: 0.005\n",
      "[Epoch 37, Step    30] loss: 0.010\n",
      "[Epoch 37, Step    40] loss: 0.007\n",
      "[Epoch 37, Step    50] loss: 0.004\n",
      "[Epoch 37, Step    60] loss: 0.037\n",
      "[Epoch 37, Step    70] loss: 0.009\n",
      "[Epoch 37, Step    80] loss: 0.006\n",
      "[Epoch 37, Step    90] loss: 0.004\n",
      "[Epoch 37, Step   100] loss: 0.014\n",
      "[Epoch 37, Step   110] loss: 0.011\n",
      "[Epoch 37, Step   120] loss: 0.020\n",
      "[Epoch 37, Step   130] loss: 0.009\n",
      "[Epoch 38, Step    10] loss: 0.004\n",
      "[Epoch 38, Step    20] loss: 0.007\n",
      "[Epoch 38, Step    30] loss: 0.009\n",
      "[Epoch 38, Step    40] loss: 0.007\n",
      "[Epoch 38, Step    50] loss: 0.014\n",
      "[Epoch 38, Step    60] loss: 0.005\n",
      "[Epoch 38, Step    70] loss: 0.008\n",
      "[Epoch 38, Step    80] loss: 0.009\n",
      "[Epoch 38, Step    90] loss: 0.010\n",
      "[Epoch 38, Step   100] loss: 0.005\n",
      "[Epoch 38, Step   110] loss: 0.013\n",
      "[Epoch 38, Step   120] loss: 0.007\n",
      "[Epoch 38, Step   130] loss: 0.007\n",
      "[Epoch 39, Step    10] loss: 0.004\n",
      "[Epoch 39, Step    20] loss: 0.005\n",
      "[Epoch 39, Step    30] loss: 0.009\n",
      "[Epoch 39, Step    40] loss: 0.009\n",
      "[Epoch 39, Step    50] loss: 0.008\n",
      "[Epoch 39, Step    60] loss: 0.008\n",
      "[Epoch 39, Step    70] loss: 0.003\n",
      "[Epoch 39, Step    80] loss: 0.012\n",
      "[Epoch 39, Step    90] loss: 0.006\n",
      "[Epoch 39, Step   100] loss: 0.003\n",
      "[Epoch 39, Step   110] loss: 0.005\n",
      "[Epoch 39, Step   120] loss: 0.004\n",
      "[Epoch 39, Step   130] loss: 0.013\n",
      "[Epoch 40, Step    10] loss: 0.006\n",
      "[Epoch 40, Step    20] loss: 0.007\n",
      "[Epoch 40, Step    30] loss: 0.010\n",
      "[Epoch 40, Step    40] loss: 0.005\n",
      "[Epoch 40, Step    50] loss: 0.005\n",
      "[Epoch 40, Step    60] loss: 0.005\n",
      "[Epoch 40, Step    70] loss: 0.004\n",
      "[Epoch 40, Step    80] loss: 0.006\n",
      "[Epoch 40, Step    90] loss: 0.007\n",
      "[Epoch 40, Step   100] loss: 0.006\n",
      "[Epoch 40, Step   110] loss: 0.004\n",
      "[Epoch 40, Step   120] loss: 0.008\n",
      "[Epoch 40, Step   130] loss: 0.011\n",
      "[Epoch 41, Step    10] loss: 0.004\n",
      "[Epoch 41, Step    20] loss: 0.003\n",
      "[Epoch 41, Step    30] loss: 0.006\n",
      "[Epoch 41, Step    40] loss: 0.008\n",
      "[Epoch 41, Step    50] loss: 0.005\n",
      "[Epoch 41, Step    60] loss: 0.004\n",
      "[Epoch 41, Step    70] loss: 0.009\n",
      "[Epoch 41, Step    80] loss: 0.004\n",
      "[Epoch 41, Step    90] loss: 0.004\n",
      "[Epoch 41, Step   100] loss: 0.006\n",
      "[Epoch 41, Step   110] loss: 0.015\n",
      "[Epoch 41, Step   120] loss: 0.005\n",
      "[Epoch 41, Step   130] loss: 0.011\n",
      "[Epoch 42, Step    10] loss: 0.006\n",
      "[Epoch 42, Step    20] loss: 0.006\n",
      "[Epoch 42, Step    30] loss: 0.002\n",
      "[Epoch 42, Step    40] loss: 0.008\n",
      "[Epoch 42, Step    50] loss: 0.017\n",
      "[Epoch 42, Step    60] loss: 0.003\n",
      "[Epoch 42, Step    70] loss: 0.005\n",
      "[Epoch 42, Step    80] loss: 0.006\n",
      "[Epoch 42, Step    90] loss: 0.006\n",
      "[Epoch 42, Step   100] loss: 0.002\n",
      "[Epoch 42, Step   110] loss: 0.006\n",
      "[Epoch 42, Step   120] loss: 0.004\n",
      "[Epoch 42, Step   130] loss: 0.004\n",
      "[Epoch 43, Step    10] loss: 0.002\n",
      "[Epoch 43, Step    20] loss: 0.010\n",
      "[Epoch 43, Step    30] loss: 0.004\n",
      "[Epoch 43, Step    40] loss: 0.005\n",
      "[Epoch 43, Step    50] loss: 0.011\n",
      "[Epoch 43, Step    60] loss: 0.005\n",
      "[Epoch 43, Step    70] loss: 0.005\n",
      "[Epoch 43, Step    80] loss: 0.002\n",
      "[Epoch 43, Step    90] loss: 0.004\n",
      "[Epoch 43, Step   100] loss: 0.005\n",
      "[Epoch 43, Step   110] loss: 0.003\n",
      "[Epoch 43, Step   120] loss: 0.004\n",
      "[Epoch 43, Step   130] loss: 0.003\n",
      "[Epoch 44, Step    10] loss: 0.003\n",
      "[Epoch 44, Step    20] loss: 0.005\n",
      "[Epoch 44, Step    30] loss: 0.006\n",
      "[Epoch 44, Step    40] loss: 0.006\n",
      "[Epoch 44, Step    50] loss: 0.003\n",
      "[Epoch 44, Step    60] loss: 0.008\n",
      "[Epoch 44, Step    70] loss: 0.003\n",
      "[Epoch 44, Step    80] loss: 0.003\n",
      "[Epoch 44, Step    90] loss: 0.008\n",
      "[Epoch 44, Step   100] loss: 0.003\n",
      "[Epoch 44, Step   110] loss: 0.002\n",
      "[Epoch 44, Step   120] loss: 0.006\n",
      "[Epoch 44, Step   130] loss: 0.005\n",
      "[Epoch 45, Step    10] loss: 0.004\n",
      "[Epoch 45, Step    20] loss: 0.004\n",
      "[Epoch 45, Step    30] loss: 0.008\n",
      "[Epoch 45, Step    40] loss: 0.005\n",
      "[Epoch 45, Step    50] loss: 0.006\n",
      "[Epoch 45, Step    60] loss: 0.002\n",
      "[Epoch 45, Step    70] loss: 0.004\n",
      "[Epoch 45, Step    80] loss: 0.001\n",
      "[Epoch 45, Step    90] loss: 0.007\n",
      "[Epoch 45, Step   100] loss: 0.005\n",
      "[Epoch 45, Step   110] loss: 0.002\n",
      "[Epoch 45, Step   120] loss: 0.001\n",
      "[Epoch 45, Step   130] loss: 0.007\n",
      "[Epoch 46, Step    10] loss: 0.003\n",
      "[Epoch 46, Step    20] loss: 0.005\n",
      "[Epoch 46, Step    30] loss: 0.003\n",
      "[Epoch 46, Step    40] loss: 0.003\n",
      "[Epoch 46, Step    50] loss: 0.001\n",
      "[Epoch 46, Step    60] loss: 0.006\n",
      "[Epoch 46, Step    70] loss: 0.004\n",
      "[Epoch 46, Step    80] loss: 0.004\n",
      "[Epoch 46, Step    90] loss: 0.004\n",
      "[Epoch 46, Step   100] loss: 0.005\n",
      "[Epoch 46, Step   110] loss: 0.007\n",
      "[Epoch 46, Step   120] loss: 0.003\n",
      "[Epoch 46, Step   130] loss: 0.002\n",
      "[Epoch 47, Step    10] loss: 0.003\n",
      "[Epoch 47, Step    20] loss: 0.005\n",
      "[Epoch 47, Step    30] loss: 0.002\n",
      "[Epoch 47, Step    40] loss: 0.003\n",
      "[Epoch 47, Step    50] loss: 0.003\n",
      "[Epoch 47, Step    60] loss: 0.003\n",
      "[Epoch 47, Step    70] loss: 0.002\n",
      "[Epoch 47, Step    80] loss: 0.006\n",
      "[Epoch 47, Step    90] loss: 0.004\n",
      "[Epoch 47, Step   100] loss: 0.003\n",
      "[Epoch 47, Step   110] loss: 0.006\n",
      "[Epoch 47, Step   120] loss: 0.002\n",
      "[Epoch 47, Step   130] loss: 0.007\n",
      "[Epoch 48, Step    10] loss: 0.002\n",
      "[Epoch 48, Step    20] loss: 0.004\n",
      "[Epoch 48, Step    30] loss: 0.002\n",
      "[Epoch 48, Step    40] loss: 0.004\n",
      "[Epoch 48, Step    50] loss: 0.005\n",
      "[Epoch 48, Step    60] loss: 0.005\n",
      "[Epoch 48, Step    70] loss: 0.002\n",
      "[Epoch 48, Step    80] loss: 0.002\n",
      "[Epoch 48, Step    90] loss: 0.003\n",
      "[Epoch 48, Step   100] loss: 0.003\n",
      "[Epoch 48, Step   110] loss: 0.002\n",
      "[Epoch 48, Step   120] loss: 0.005\n",
      "[Epoch 48, Step   130] loss: 0.004\n",
      "[Epoch 49, Step    10] loss: 0.002\n",
      "[Epoch 49, Step    20] loss: 0.001\n",
      "[Epoch 49, Step    30] loss: 0.002\n",
      "[Epoch 49, Step    40] loss: 0.002\n",
      "[Epoch 49, Step    50] loss: 0.006\n",
      "[Epoch 49, Step    60] loss: 0.004\n",
      "[Epoch 49, Step    70] loss: 0.005\n",
      "[Epoch 49, Step    80] loss: 0.006\n",
      "[Epoch 49, Step    90] loss: 0.003\n",
      "[Epoch 49, Step   100] loss: 0.002\n",
      "[Epoch 49, Step   110] loss: 0.006\n",
      "[Epoch 49, Step   120] loss: 0.003\n",
      "[Epoch 49, Step   130] loss: 0.002\n",
      "[Epoch 50, Step    10] loss: 0.002\n",
      "[Epoch 50, Step    20] loss: 0.003\n",
      "[Epoch 50, Step    30] loss: 0.003\n",
      "[Epoch 50, Step    40] loss: 0.001\n",
      "[Epoch 50, Step    50] loss: 0.002\n",
      "[Epoch 50, Step    60] loss: 0.001\n",
      "[Epoch 50, Step    70] loss: 0.004\n",
      "[Epoch 50, Step    80] loss: 0.003\n",
      "[Epoch 50, Step    90] loss: 0.002\n",
      "[Epoch 50, Step   100] loss: 0.004\n",
      "[Epoch 50, Step   110] loss: 0.002\n",
      "[Epoch 50, Step   120] loss: 0.003\n",
      "[Epoch 50, Step   130] loss: 0.003\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# define the initial learning rate here\n",
    "learning_rate = 1e-2\n",
    "n_epochs = 50 # how many epochs to run\n",
    "\n",
    "\n",
    "# define loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "cnn_net = Net().cuda()\n",
    "optimizer = torch.optim.SGD(cnn_net.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        labels = labels.float().cuda()\n",
    "        \n",
    "\n",
    "        # Forward \n",
    "        output = cnn_net(inputs.cuda())\n",
    "        \n",
    "        # Compute the loss using the final output\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        # YOUR CODE HERE\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:  # print every 10 mini-batches\n",
    "            print('[Epoch %d, Step %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0168c191-d190-4521-a16f-3af096a28262",
   "metadata": {},
   "source": [
    "**Evaluation**: Now testing with your trained model on the all test datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8d389ce-e210-4547-a074-f7a3018924ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 96 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# since you're not training, you don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        _, labels = torch.max(labels, 1)\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        output = cnn_net(images)\n",
    "\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8d96a-c844-4b6f-a24c-ca3e5c4a3c2c",
   "metadata": {},
   "source": [
    "**Evaluation**: Testing with your trained model on the each labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d04a9153-a69c-4a4d-ac92-8085b879c7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class panda is: 98.0 %\n",
      "Accuracy for class grizzly is: 94.7 %\n"
     ]
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        _, labels = torch.max(labels, 1)\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        output = cnn_net(images)\n",
    "        _, predictions = torch.max(output, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                         accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
